{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/coartix/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83e1df53fc7463e94e23ad2a014919c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The dataset has 3 splits\n",
    "\n",
    "2. The train and test splits have 25000 rows, and the unsupervised split has 50000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of positive labels in train split:  0.5\n",
      "Proportion of positive labels in test split:  0.5\n",
      "Proportion of negative labels in train split:  0.5\n",
      "Proportion of negative labels in test split:  0.5\n"
     ]
    }
   ],
   "source": [
    "pos_train = sum(dataset[\"train\"][\"label\"]) / len(dataset[\"train\"][\"label\"])\n",
    "pos_test = sum(dataset[\"test\"][\"label\"]) / len(dataset[\"test\"][\"label\"])\n",
    "neg_train = 1 - pos_train\n",
    "neg_test = 1 - pos_test\n",
    "\n",
    "print(\"Proportion of positive labels in train split: \", pos_train)\n",
    "print(\"Proportion of positive labels in test split: \", pos_test)\n",
    "print(\"Proportion of negative labels in train split: \", neg_train)\n",
    "print(\"Proportion of negative labels in test split: \", neg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 50% of positive/negative label are in the train split and test plit.\n",
    "\n",
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an adpated preprocessing function which at least lowers the text and replace punctuations with spaces\n",
    "# You can use from string import punctuation to get a list of punctuations, maybe not all punctuations should be removed\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_function(dataset):\n",
    "    '''\n",
    "        Input: dataset\n",
    "        Output: train_docs, test_docs, train_classes, test_classes\n",
    "    '''\n",
    "    punct = punctuation.replace(\"-\", \"\")\n",
    "    # lower and change punctuation into spaces and replace multiple spaces for only one\n",
    "    train_docs = np.array([text.lower().translate(str.maketrans(punct, \" \" * len(punct))) for text in dataset[\"train\"][\"text\"]])\n",
    "    test_docs = np.array([text.lower().translate(str.maketrans(punct, \" \" * len(punct))) for text in dataset[\"train\"][\"text\"]])\n",
    "    train_classes = np.array(dataset[\"train\"][\"label\"])\n",
    "    test_classes = np.array(dataset[\"test\"][\"label\"])\n",
    "    return train_docs, test_docs, train_classes, test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def train_naive_bayes(docs: np.ndarray, classes: np.ndarray):\n",
    "    '''\n",
    "        Input: docs, classes\n",
    "        Output: log_prior, log_likelihood, vocab\n",
    "    '''\n",
    "    # Get each word uniquely in the docs\n",
    "    vocab = np.unique(np.concatenate([text.split() for text in docs]))\n",
    "    \n",
    "    Ndoc = len(docs)\n",
    "\n",
    "    # Get the number of different classes\n",
    "    unique_classes = np.unique(classes)\n",
    "    for c in unique_classes:\n",
    "        Nc = sum(classes == c)\n",
    "\n",
    "        # Compute the log prior probability of class c\n",
    "        log_prior = math.log(Nc / Ndoc)\n",
    "        \n",
    "        # create bigdoc which is an histogram of the words in the docs of class c\n",
    "        bigdoc = np.concatenate([text.split() for text in docs[classes == c]])\n",
    "        histo = np.array([sum(bigdoc == word) for word in vocab])\n",
    "\n",
    "        # Compute the log likelihood of each word in the vocabulary for class c\n",
    "        log_likelihood = np.log((histo + 1) / (len(bigdoc) + len(vocab)))\n",
    "        print('Half done')\n",
    "    \n",
    "    return log_prior, log_likelihood, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_doc: str, log_prior: float, log_likelihood: np.ndarray, unique_test_classes: np.ndarray, vocab: np.ndarray):\n",
    "    '''\n",
    "        Input: test_doc, log_prior, log_likelihood, test_classes, vocab\n",
    "        Output: accuracy\n",
    "    '''\n",
    "    sums = np.zeros(len(unique_test_classes))\n",
    "    for c in unique_test_classes:\n",
    "        # Compute the log posterior probability of class c\n",
    "        log_posterior = log_prior + sum(log_likelihood[np.where(np.isin(vocab, test_doc.split()))])\n",
    "        sums[c] = log_posterior\n",
    "    return np.argmax(sums)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,) (25000,) (25000,)\n",
      "(95669,)\n"
     ]
    }
   ],
   "source": [
    "# Call the preprocessing function\n",
    "train_docs, test_docs, train_classes, test_classes = preprocess_function(dataset)\n",
    "print(train_docs.shape, test_docs.shape, train_classes.shape, test_classes.shape)\n",
    "vocab = np.unique(np.concatenate([text.split() for text in train_docs]))\n",
    "print(vocab.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7528\n",
      "ok\n",
      "yes\n",
      "no\n",
      "0.0 (7528,) (7528,)\n"
     ]
    }
   ],
   "source": [
    "# Train the model with a hundrendth of the training data\n",
    "log_prior, log_likelihood, vocab = train_naive_bayes(train_docs[:int(len(train_docs)/100)], train_classes[:int(len(train_classes)/100)])\n",
    "print(log_prior, log_likelihood.shape, vocab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Test the model with for 1000 test data\n",
    "unique_test_classes = np.unique(test_classes)\n",
    "accuracy = 0\n",
    "for i in range(25000):\n",
    "    accuracy += test_naive_bayes(test_docs[i], log_prior, log_likelihood, unique_test_classes, vocab) == test_classes[i]\n",
    "accuracy /= 25000\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
