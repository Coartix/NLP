{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TP1 done by Litoux Pierre, Arsenec Charles-AndrÃ©, Deplagne Hugo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import functools\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How many splits does the dataset has?\n",
    "The dataset has 3 splits (training, test, unsupervised).\n",
    "\n",
    "### 2. How big are these splits?\n",
    "train -> 25000 rows  \n",
    "test -> 25000 rows  \n",
    "unsupervised -> 50000 rows  \n",
    "A row contains a text and a value 0 or 1 corresponding respectively to a negative sentiment or a positive one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will create dataframes to manipulate the data in each splits :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(dataset['test'])\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unsupervised = pd.DataFrame(dataset['unsupervised'])\n",
    "df_unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the proportion of each class on the supervised splits?\n",
    "\n",
    "Here is a plot counting the number of each class into train and test splits :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "for i, df, df_str in zip([0, 1], [df_train, df_test], ['df_train', 'df_test']):\n",
    "    ax[i].set_xticks([0, 1])\n",
    "    ax[i].set_yticks([0, df['label'].value_counts().max()])\n",
    "    ax[i].hist(df['label'])\n",
    "    ax[i].set_xlabel('label')\n",
    "    ax[i].set_ylabel('count')\n",
    "    ax[i].set_title(df_str)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_train[\"label\"].unique()\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are either 0 or 1. Each classes count up for 12500 on each splits (test and train)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will lower the text to not duplicate words that may be the same with capitals.  \n",
    "Some texts contain some html like \"<br \\/>\" or punctuations which are not necessary. <br> Let remove them except for '-' which may be useful to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingString(text: str) -> str:\n",
    "    '''\n",
    "        Preprocessing string\n",
    "        Input:\n",
    "            text: string\n",
    "        Output:\n",
    "            text: string\n",
    "    '''\n",
    "    text = text.lower().replace(\"<br />\", \" \")\n",
    "    for punct in punctuation:\n",
    "        if (not punct in str(\"-\")):\n",
    "            text = text.replace(punct, \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(lambda text: preprocessingString(text))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text'] = df_test['text'].apply(lambda text: preprocessingString(text))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Naive Bayes from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNaiveBayes(df: pd.DataFrame, classes: list) -> tuple:\n",
    "    '''\n",
    "        Input:\n",
    "            df: dataframe\n",
    "            classes: list of classes\n",
    "        Output:\n",
    "            log_prior: list of log prior of each class\n",
    "            loglikehood: list of log likehood of each class\n",
    "            vocabulary: set of vocabulary\n",
    "    '''\n",
    "    log_prior = []\n",
    "    loglikehood = []\n",
    "    vocabulary = set(str().join((df['text'])).split(\" \"))\n",
    "    n_doc = df.shape[0]\n",
    "    \n",
    "    for class_c in classes:\n",
    "        n_class = df[df['label'] == class_c].shape[0]\n",
    "        log_prior.append(np.log(n_class / n_doc))\n",
    "        big_doc = str().join((df[df['label'] == class_c]['text'])).split(\" \")\n",
    "        \n",
    "        d = defaultdict(int)\n",
    "        for word in big_doc:\n",
    "           d[word] += 1 \n",
    "        sumcount_v = sum(d.values()) + len(vocabulary)\n",
    "        \n",
    "        loglikehood_c = {}\n",
    "        for word in vocabulary:\n",
    "            loglikehood_c[word] = np.log((d[word] + 1) / sumcount_v)\n",
    "        loglikehood.append(loglikehood_c)\n",
    "    \n",
    "    return log_prior, loglikehood, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior, loglikehood, vocabulary = trainNaiveBayes(df_train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testNaiveBayes(testdoc: str, logprior: list, loglikehood: list, classes: list, vocabulary: set) -> int:\n",
    "    '''\n",
    "        Input:\n",
    "            testdoc: string\n",
    "            log_prior: list of log prior of each class\n",
    "            loglikehood: list of log likehood of each class\n",
    "            classes: list of classes\n",
    "            vocabulary: set of vocabulary\n",
    "        Output:\n",
    "            class: class of testdoc\n",
    "    '''\n",
    "    probabilty_class = {}\n",
    "    for classes_c in classes:\n",
    "        probabilty_class[classes_c] = logprior[classes_c]\n",
    "        for word in testdoc.split():\n",
    "            if (word in vocabulary):\n",
    "                probabilty_class[classes_c] += loglikehood[classes_c][word]\n",
    "    return np.argmax(list(probabilty_class.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = df_test['text'].apply(lambda text : testNaiveBayes(text, logprior, loglikehood, labels, vocabulary))\n",
    "y_true = df_test['label']\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Naive Bayes from Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "PipelineNB = Pipeline([('vect', CountVectorizer()),\n",
    "                       ('clf', MultinomialNB())])\n",
    "PipelineNB.fit(df_train['text'], df_train['label'])\n",
    "\n",
    "y_pred = PipelineNB.predict(df_test['text'])\n",
    "y_true = df_test['label']\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Accuracy report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own implementation\n",
    "# Training set\n",
    "y_pred = df_train['text'].apply(lambda text : testNaiveBayes(text, logprior, loglikehood, labels, vocabulary))\n",
    "y_true = df_train['label']\n",
    "print(\"Own implementation - Training set accuracy: \", accuracy_score(y_true, y_pred))\n",
    "\n",
    "# Test set\n",
    "y_pred = df_test['text'].apply(lambda text : testNaiveBayes(text, logprior, loglikehood, labels, vocabulary))\n",
    "y_true = df_test['label']\n",
    "print(\"Own implementation - Test set accuracy: \", accuracy_score(y_true, y_pred))\n",
    "\n",
    "# Scikit-learn implementation\n",
    "# Training set\n",
    "y_pred = PipelineNB.predict(df_train['text'])\n",
    "y_true = df_train['label']\n",
    "print(\"Scikit-learn implementation - Training set accuracy: \", accuracy_score(y_true, y_pred))\n",
    "\n",
    "# Test set\n",
    "y_pred = PipelineNB.predict(df_test['text'])\n",
    "y_true = df_test['label']\n",
    "print(\"Scikit-learn implementation - Test set accuracy: \", accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.suptitle('Accuracy', fontsize=16)\n",
    "\n",
    "for i, cm in zip([1, 2], [cm, cmn]):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    ax = plt.subplot(1, 2, i)\n",
    "    disp.plot(ax=ax)\n",
    "    \n",
    "fig.tight_layout(pad=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Most likely, the scikit-learn implementation will give better results. Looking at the documentation, explain why it could be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could be explained because the multinomial Naive Bayes classifier is better suited for classification with discrete features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Why is accuracy a sufficient measure of evaluation here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there is only 2 classes so on a test the prediction is either true or false. This is well represented by a proportion of valid and wrong tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Using one of the implementation, take at least 2 wrongly classified example from the test set and try explaining why the model failed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use our own implementation and show two texts where the implementation failed to predict the right class :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = df_test['text'].apply(lambda text : testNaiveBayes(text, logprior, loglikehood, labels, vocabulary))\n",
    "y_true = df_test['label']\n",
    "\n",
    "df_wrong_pred = df_test[y_true != y_pred]\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_wrong_pred.iloc[[0, -1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is two examples where the program should have return a class 0 and 1 (negative, positive).  \n",
    "  \n",
    "-> The first one is ... \n",
    "  \n",
    "-> The second text is ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. [BONUS] What are the top 10 most important words (features) for each class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most important words for each class\n",
    "def getImportantWords(loglikehood, classes, vocabulary):\n",
    "    '''\n",
    "        Input:\n",
    "            loglikehood: list of log likehood of each class\n",
    "            classes: list of classes\n",
    "            vocabulary: set of vocabulary\n",
    "        Output:\n",
    "            important_words: dictionary of important words for each class\n",
    "    '''\n",
    "    important_words = {}\n",
    "    for classes_c in classes:\n",
    "        important_words[classes_c] = sorted(loglikehood[classes_c], key=loglikehood[classes_c].get, reverse=True)[:100]\n",
    "    return important_words\n",
    "\n",
    "importantWords = getImportantWords(loglikehood, labels, vocabulary)\n",
    "for classes_c in labels:\n",
    "    print(\"Class: \", classes_c)\n",
    "    print(importantWords[classes_c])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def removeStopWords(importantWords):\n",
    "    '''\n",
    "        Input:\n",
    "            importantWords: dictionary of important words for each class\n",
    "        Output:\n",
    "            importantWords: dictionary of important words for each class\n",
    "    '''\n",
    "    for classes_c in labels:\n",
    "        importantWords[classes_c] = [word for word in importantWords[classes_c] if word not in stops]\n",
    "    return importantWords\n",
    "\n",
    "importantWords = removeStopWords(importantWords)\n",
    "for classes_c in labels:\n",
    "    print(\"Class: \", classes_c)\n",
    "    print(importantWords[classes_c])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the most important words for each class that brings the decision to likely select the class it corresponds to. In class 1 we can see that the word 'love' or 'great' makes the decision favorable towards this class because it doesnt appear in the class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Adding stemming to pretreatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def stemmingPreProcessing(text):\n",
    "    '''\n",
    "        Input:\n",
    "            text: string\n",
    "        Output:\n",
    "            text: string after stemming\n",
    "    '''\n",
    "    re_word = re.compile(r\"^\\w+$\")\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed = [stemmer.stem(word) for word in word_tokenize(text.lower().replace(\"<br />\", \"\")) if re_word.match(word)]\n",
    "    return \" \".join(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the train dataframe after stemming\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_train['text'] = df_train['text'].apply(lambda text: stemmingPreProcessing(text))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the test dataframe after stemming\n",
    "df_test = pd.DataFrame(dataset['test'])\n",
    "df_test['text'] = df_test['text'].apply(lambda text: stemmingPreProcessing(text))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train and evaluate your model again with these pretreatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the stemmed data\n",
    "logprior, loglikehood, vocabulary = trainNaiveBayes(df_train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the train set\n",
    "y_pred = df_train['text'].apply(lambda text : testNaiveBayes(text, logprior, loglikehood, labels, vocabulary))\n",
    "y_true = df_train['label']\n",
    "print(\"Own implementation (Stemming) - Training set accuracy: \", accuracy_score(y_true, y_pred))\n",
    "\n",
    "# Testing the test set\n",
    "y_pred = df_test['text'].apply(lambda text : testNaiveBayes(text, logprior, loglikehood, labels, vocabulary))\n",
    "y_true = df_test['label']\n",
    "print(\"Own implementation (Stemming) - Test set accuracy: \", accuracy_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Are the results better or worse? Try explaining why the accuracy changed.  \n",
    "  \n",
    "The results show the stemming process to be less effective.  \n",
    "The accuracy changed because the stemming pretreatment shorten words so that we don't have multiple occurence of the same word which could take multiple forms like 'enjoy', 'enjoys', 'enjoyed', 'enjoying'. Those are all grouped and written as 'enjoy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importantWords = getImportantWords(loglikehood, labels, vocabulary)\n",
    "importantWords = removeStopWords(importantWords)\n",
    "for classes_c in labels:\n",
    "    print(\"Class: \", classes_c)\n",
    "    print(importantWords[classes_c])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
