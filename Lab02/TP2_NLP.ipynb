{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 :  Logistic regression for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/coartix/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc08765164d44a7eb7207f663756825a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_test = pd.DataFrame(dataset['test'])\n",
    "df_unsupervised = pd.DataFrame(dataset['unsupervised'])\n",
    "labels = df_train[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingString(text: str) -> str:\n",
    "    '''\n",
    "        Preprocessing string\n",
    "        Input:\n",
    "            text: string\n",
    "        Output:\n",
    "            text: string\n",
    "    '''\n",
    "    text = text.lower().replace(\"<br />\", \" \")\n",
    "    for punct in punctuation:\n",
    "        if (not punct in str(\"-\")):\n",
    "            text = text.replace(punct, \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features \n",
    "\n",
    "For every given text, we want to generate a vector with some featues.\n",
    "\n",
    "- 1 if \"no\" appears in the document, 0 otherwise.\n",
    "- The count of first and second pronouns in the document.\n",
    "- 1 if \"!\" is in the document, 0 otherwise.\n",
    "- Log(word count in the document).\n",
    "- Number of words in the document which are in the positive lexicon.\n",
    "- Number of words in the document which are in the negative lexicon.\n",
    "\n",
    "For positive and negative lexicons, you can use the resources provided by VADER sentiment. Look for the vader_lexicon.txt file and consider positive word if they score above a certain threshold (for example 1) and negative word if they score below a certain threshold (for example -1). Feel free to use another lexicon if you find one, but make sure you document your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLexicon(lexiconFile):\n",
    "    lexicon = {}\n",
    "    with open(lexiconFile, 'r') as f:\n",
    "        for line in f:\n",
    "            word, score = line.split('\\t')[:2]\n",
    "            if float(score) >= 1:\n",
    "                lexicon[word] = 1\n",
    "            elif float(score) <= -1:\n",
    "                lexicon[word] = 0\n",
    "    return lexicon\n",
    "\n",
    "lexicon = parseLexicon('vader_lexicon.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The getFeature function outputs a vector containing the 6 features needed and 1 bonus feature wich is the number of negation words.\n",
    "\n",
    "A negation word is a word wich negate a sentance like \"not\" or \"don't\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "negateWords = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    "     \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    "     \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    "     \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "     \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    "     \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    "     \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    "     \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
    "\n",
    "def getFeatures(text, lexicon):\n",
    "    features = []\n",
    "    features.append(1 if \"no\" in text else 0)\n",
    "    features.append(text.count(\"i\") + text.count('you'))\n",
    "    features.append(1 if \"!\" in text else 0)\n",
    "    features.append(math.log(len(text.split())))\n",
    "    nb_pos = 0\n",
    "    nb_neg = 0\n",
    "    nb_negate = 0\n",
    "    for word in text.split():\n",
    "        if word in lexicon:\n",
    "            if lexicon[word] == 1:\n",
    "                nb_pos += 1\n",
    "            else:\n",
    "                nb_neg += 1\n",
    "        if word in negateWords:\n",
    "            nb_negate += 1\n",
    "    features.append(nb_pos)\n",
    "    features.append(nb_neg)\n",
    "    #Bonus\n",
    "    features.append(nb_negate)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(preprocessingString)\n",
    "df_test['text'] = df_test['text'].apply(preprocessingString)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We regroup our features into the dataframes `df_train_features` and `df_test_features` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandasToFeatures(df):\n",
    "    arr = df[\"text\"].apply(lambda x : getFeatures(x, lexicon)).values\n",
    "    index = np.arange(len(df_train['text'].values))\n",
    "    columns=['contains_no', 'count_pronouns', 'contains!', 'log_wordcount', 'nbr_positive', 'nbr_negative', 'negate_word']\n",
    "    \n",
    "    df_features = pd.DataFrame.from_dict(data=dict(zip(index, arr)), orient='index', columns=columns)\n",
    "    df_features['label'] = df['label']\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contains_no</th>\n",
       "      <th>count_pronouns</th>\n",
       "      <th>contains!</th>\n",
       "      <th>log_wordcount</th>\n",
       "      <th>nbr_positive</th>\n",
       "      <th>nbr_negative</th>\n",
       "      <th>negate_word</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>5.662960</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>5.407172</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>4.510860</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>4.795791</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>5.743003</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   contains_no  count_pronouns  contains!  log_wordcount  nbr_positive  \\\n",
       "0            1             100          0       5.662960             7   \n",
       "1            1              87          0       5.407172             6   \n",
       "2            1              43          0       4.510860             3   \n",
       "3            1              50          0       4.795791             5   \n",
       "4            1             112          0       5.743003             4   \n",
       "\n",
       "   nbr_negative  negate_word  label  \n",
       "0             6            1      0  \n",
       "1             4            4      0  \n",
       "2             3            1      0  \n",
       "3             5            1      0  \n",
       "4            11            1      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_features = pandasToFeatures(df_train)\n",
    "df_test_features = pandasToFeatures(df_test)\n",
    "df_train_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18000, 7])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_col = df_train_features.columns[:-1]\n",
    "all_points = torch.tensor(df_train_features[train_col].values, dtype=torch.float32)\n",
    "labels = torch.tensor(df_train_features['label'].values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_points,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    stratify=labels,\n",
    "    random_state=42,\n",
    ")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.1,\n",
    "    stratify=y_train,\n",
    "    random_state=42,\n",
    ")\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    \"\"\"A logistic regression implementation\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, nb_classes: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: the dimension of the input features.\n",
    "            nb_classes: the number of classes to predict.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        output_layer = nn.Sigmoid() if nb_classes == 1 else nn.Softmax()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(input_dim, nb_classes),\n",
    "            output_layer,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: the input tensor.\n",
    "        Returns:\n",
    "            The output of activation function.\n",
    "        \"\"\"\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(7, 1)\n",
    "criterion = nn.BCELoss()  # Binary cross entropy\n",
    "# Stochastic gradient descent\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9005, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5740, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5753, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5759, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5760, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "CPU times: user 16.5 s, sys: 31.7 ms, total: 16.5 s\n",
      "Wall time: 4.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def train(model, criterion, optimizer, X_train, y_train, X_valid, y_valid, n_epochs=1000, pretty_print=True):\n",
    "    # Keeping an eye on the losses\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        # Setting all gradients to zero.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Sending the whole training set through the model.\n",
    "        predictions = model(X_train)\n",
    "        # Computing the loss.\n",
    "        loss = criterion(predictions, y_train)\n",
    "        train_losses.append(loss.item())\n",
    "        if epoch % 1000 == 0 and pretty_print:\n",
    "            print(loss)\n",
    "        # Computing the gradients and gradient descent.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # When computing the validation loss, we do not want to update the weights.\n",
    "        # torch.no_grad tells PyTorch to not save the necessary data used for\n",
    "        # gradient descent.\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_valid)\n",
    "            loss = criterion(predictions, y_valid)\n",
    "            test_losses.append(loss)\n",
    "    return train_losses, test_losses\n",
    "\n",
    "train_losses, test_losses = train(model, criterion, optimizer, X_train, y_train, X_valid, y_valid, n_epochs=5000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss results\n",
    "\n",
    "First training loss:  0.9004537463188171  \n",
    "Final training loss:  0.5760501027107239  \n",
    "First test loss:  tensor(0.6164)  \n",
    "Final test loss:  tensor(0.5801)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f45050668c0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking the losses\n",
    "plt.plot(np.arange(len(train_losses)), train_losses, label=\"Training loss\")\n",
    "plt.plot(np.arange(len(test_losses)), test_losses, label=\"Test loss\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate your classifier in terms of accuracy for the training, validation, and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7267222222222223 0.7025 0.7132\n"
     ]
    }
   ],
   "source": [
    "# Computing the accuracy for our 3 splits.\n",
    "with torch.no_grad():\n",
    "    p_train = model(X_train)\n",
    "    p_train = np.round(p_train.numpy())\n",
    "    training_accuracy = np.mean(p_train == y_train.numpy())\n",
    "    p_valid = model(X_valid)\n",
    "    p_valid = np.round(p_valid.numpy())\n",
    "    valid_accuracy = np.mean(p_valid == y_valid.numpy())\n",
    "    p_test = model(X_test)\n",
    "    p_test = np.round(p_test.numpy())\n",
    "    test_accuracy = np.mean(p_test == y_test.numpy())\n",
    "print(training_accuracy, valid_accuracy, test_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results :\n",
    "training accuracy : 0.726\n",
    "validation accuracy : 0.7025\n",
    "test acccuracy : 0.7132\n",
    "\n",
    "We can see that the classifier is more accurate than random on all the datasets although it is less accurate than our Naive Bayes Classifier. \n",
    "The validation set accuracy and the testing set accuracy are quite close so our model does not overfit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the weights of your classifier. Which features seems to play most for both classes ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0082,  0.0075,  0.0116, -0.0505,  0.1020, -0.1755, -0.0958]]),\n",
       " tensor([0.0127]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the model's weights and bias.\n",
    "model.classifier[0].state_dict()[\"weight\"], model.classifier[0].state_dict()[\"bias\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key feature used to detect if a sentence is positive is the number of words in the document wich are in the positive lexicon.\n",
    "\n",
    "Not so surprising is that the key feature to detect if a sentence is negative is the number of words in the document which are in the negative lexicon."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Bonus] The parameter weight_decay of the SGD optimizer corresponds to the L2 penalty. Try playing with this value and explain how it influence the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight decay:  0.01\n",
      "tensor([[ 0.0348,  0.0092, -0.1278, -0.1362,  0.1339, -0.2038, -0.0442]]) tensor([-0.0464])\n",
      "Weight decay:  0.1\n",
      "tensor([[-0.0602,  0.0062, -0.0593,  0.0596,  0.1052, -0.1879, -0.1111]]) tensor([-0.2799])\n",
      "Weight decay:  0.5\n",
      "tensor([[-0.1511,  0.0083,  0.0306, -0.0931,  0.1153, -0.1610, -0.1859]]) tensor([0.2105])\n",
      "Weight decay:  1\n",
      "tensor([[ 0.0209,  0.0045, -0.0109, -0.0085,  0.0897, -0.1523, -0.0266]]) tensor([-0.1434])\n",
      "Weight decay:  2\n",
      "tensor([[-0.0450,  0.0074,  0.0096, -0.0122,  0.0775, -0.1278, -0.0571]]) tensor([-0.0156])\n",
      "Weight decay:  5\n",
      "tensor([[-0.0041, -0.0056, -0.0013, -0.0116,  0.0590, -0.0955, -0.0188]]) tensor([-0.0036])\n",
      "Weight decay:  10\n",
      "tensor([[-2.1696e-03,  1.1099e-02,  1.3009e-05, -5.8495e-03,  4.3885e-02,\n",
      "         -6.2728e-02, -1.0262e-02]]) tensor([-0.0011])\n",
      "Weight decay:  20\n",
      "tensor([[-1.0573e-03,  1.1570e-02,  2.9669e-11, -2.6062e-03,  2.7620e-02,\n",
      "         -3.6591e-02, -5.2268e-03]]) tensor([-0.0005])\n",
      "Weight decay:  100\n",
      "tensor([[-3.3155e-04, -1.1361e-02,  5.6052e-45, -1.1942e-03,  4.7183e-03,\n",
      "         -9.7403e-03, -1.4199e-03]]) tensor([-0.0002])\n"
     ]
    }
   ],
   "source": [
    "# param grid for weight decay\n",
    "param_grid = {\"weight_decay\": [0.01, 0.1, 0.5, 1, 2, 5, 10, 20, 100]}\n",
    "# Grid search\n",
    "for elt in param_grid[\"weight_decay\"]:\n",
    "    model_w = LogisticRegression(7, 1)\n",
    "    criterion_w = nn.BCELoss()  # Binary cross entropy\n",
    "    optimizer_w = torch.optim.SGD(model_w.parameters(), lr=0.001, weight_decay=elt)\n",
    "    train_losses, test_losses = train(model_w, criterion_w, optimizer_w, X_train, y_train, X_valid, y_valid, n_epochs=1000, pretty_print=False)\n",
    "    print(\"Weight decay: \", elt)\n",
    "    print(model_w.classifier[0].state_dict()[\"weight\"], model_w.classifier[0].state_dict()[\"bias\"])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the weight decay parameter let the weights have much more extreme values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take two wrongly classified samples in the test set and try explaining why the model was wrong.\n",
    "\n",
    "feature meaning : [contains_no, count_pronoun, contains!, log_wordcount, nbr_positive, nbr_negative, negate_word\t]  \n",
    "feature for example 1 :  [1, 40, 0, 4.912654885736052, 12, 2, 4]  \n",
    "feature for example 2 :  [1, 47, 0, 4.882801922586371, 7, 9, 0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>ai_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>first off let me say  if you haven t enjoyed a van damme movie since bloodsport  you probably will not like this movie  most of these movies may not have the best plots or best actors but i enjoy these kinds of movies for what they are  this movie is much better than any of the movies the other action guys  segal and dolph  have thought about putting out the past few years  van damme is good in the movie  the movie is only worth watching to van damme fans  it is not as good as wake of death  which i highly recommend to anyone of likes van damme  or in hell but  in my opinion it s worth watching  it has the same type of feel to it as nowhere to run  good fun stuff</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>i caught this movie on the sci-fi channel recently  it actually turned out to be pretty decent as far as b-list horror suspense films go  two guys  one naive and one loud mouthed a    take a road trip to stop a wedding but have the worst possible luck when a maniac in a freaky  make-shift tank truck hybrid decides to play cat-and-mouse with them  things are further complicated when they pick up a ridiculously whorish hitchhiker  what makes this film unique is that the combination of comedy and terror actually work in this movie  unlike so many others  the two guys are likable enough and there are some good chase suspense scenes  nice pacing and comic timing make this movie more than passable for the horror slasher buff  definitely worth checking out</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           text  \\\n",
       "4                                                                                              first off let me say  if you haven t enjoyed a van damme movie since bloodsport  you probably will not like this movie  most of these movies may not have the best plots or best actors but i enjoy these kinds of movies for what they are  this movie is much better than any of the movies the other action guys  segal and dolph  have thought about putting out the past few years  van damme is good in the movie  the movie is only worth watching to van damme fans  it is not as good as wake of death  which i highly recommend to anyone of likes van damme  or in hell but  in my opinion it s worth watching  it has the same type of feel to it as nowhere to run  good fun stuff    \n",
       "24999  i caught this movie on the sci-fi channel recently  it actually turned out to be pretty decent as far as b-list horror suspense films go  two guys  one naive and one loud mouthed a    take a road trip to stop a wedding but have the worst possible luck when a maniac in a freaky  make-shift tank truck hybrid decides to play cat-and-mouse with them  things are further complicated when they pick up a ridiculously whorish hitchhiker  what makes this film unique is that the combination of comedy and terror actually work in this movie  unlike so many others  the two guys are likable enough and there are some good chase suspense scenes  nice pacing and comic timing make this movie more than passable for the horror slasher buff  definitely worth checking out    \n",
       "\n",
       "       label  ai_pred  \n",
       "4          0        1  \n",
       "24999      1        0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take wrong predictions and look at them\n",
    "with torch.no_grad():\n",
    "    y_pred = df_test['text'].apply(lambda text : (int)(np.round(model(torch.tensor(getFeatures(text, lexicon), dtype=torch.float32)))))\n",
    "df_wrong_pred = df_test[y_pred != df_test['label']].loc[:, ['text', 'label']]\n",
    "df_wrong_pred['ai_pred'] = y_pred.loc[y_pred!= df_test['label']]\n",
    "\n",
    "# take wrong predictions and look at their features\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_wrong_pred.iloc[[0, -1]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is two examples where the program should have return a class 0 and 1 (negative, positive).  \n",
    "\n",
    "The first one is a negative review but our model predicted it was a positive review because there is a lot of positive words in this example like \"best\" or \"like\" \"better\" \"good\" although they are used in a negative way or to compare to a better movie.\n",
    "\n",
    "The second one is a positive review but our model predicted it was negative, it is most likely because they are more negative words than positives one in this review, these negative words are used to describe the story not top criticize the movie but the model does not know that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit :  0.715 pytorch :  0.7132\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn implementation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Training the model\n",
    "clf = LogisticRegression(random_state=42, max_iter=5000)\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "\n",
    "# Checking the accuracy\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# compare pytorch and scikit-learn accuracy\n",
    "print(\"scikit : \" ,accuracy_score(y_test, y_pred),\"pytorch : \", test_accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **\\[Bonus\\]** Train logistic regression classifier using the [scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). How does it compare with the PyTorch version?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy --> scikit :  0.715 pytorch :  0.7132\n",
    "\n",
    "Both implementations seems to have the same results with the same datasets and feature vectors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea9312fc9c2d6329322094b403542e3a8436f2615ce450c7872cc3a27bdb75bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
