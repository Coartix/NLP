{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 :  Logistic regression for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/pierre/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76196569d084bef9853b0bf704be8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_test = pd.DataFrame(dataset['test'])\n",
    "df_unsupervised = pd.DataFrame(dataset['unsupervised'])\n",
    "labels = df_train[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingString(text: str) -> str:\n",
    "    '''\n",
    "        Preprocessing string\n",
    "        Input:\n",
    "            text: string\n",
    "        Output:\n",
    "            text: string\n",
    "    '''\n",
    "    text = text.lower().replace(\"<br />\", \" \")\n",
    "    for punct in punctuation:\n",
    "        if (not punct in str(\"-\")):\n",
    "            text = text.replace(punct, \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features \n",
    "\n",
    "For every given text, we want to generate a vector with some featues.\n",
    "\n",
    "- 1 if \"no\" appears in the document, 0 otherwise.\n",
    "- The count of first and second pronouns in the document.\n",
    "- 1 if \"!\" is in the document, 0 otherwise.\n",
    "- Log(word count in the document).\n",
    "- Number of words in the document which are in the positive lexicon.\n",
    "- Number of words in the document which are in the negative lexicon.\n",
    "\n",
    "For positive and negative lexicons, you can use the resources provided by VADER sentiment. Look for the vader_lexicon.txt file and consider positive word if they score above a certain threshold (for example 1) and negative word if they score below a certain threshold (for example -1). Feel free to use another lexicon if you find one, but make sure you document your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$:': 0,\n",
       " '%-)': 0,\n",
       " \"( '}{' )\": 1,\n",
       " \"('-:\": 1,\n",
       " \"(':\": 1,\n",
       " '((-:': 1,\n",
       " '(*': 1,\n",
       " '(-*': 1,\n",
       " '(-:': 1,\n",
       " '(-:0': 1,\n",
       " '(-:o': 1,\n",
       " '(-:O': 1,\n",
       " '(-:|>*': 1,\n",
       " '(-;': 1,\n",
       " '(-;|': 1,\n",
       " '(8': 1,\n",
       " '(:': 1,\n",
       " '(:0': 1,\n",
       " '(:o': 1,\n",
       " '(:O': 1,\n",
       " '(;': 1,\n",
       " '(=': 1,\n",
       " '(?:': 1,\n",
       " '(^:': 1,\n",
       " '(^;': 1,\n",
       " '(^;0': 1,\n",
       " '(^;o': 1,\n",
       " '(o:': 1,\n",
       " \")':\": 0,\n",
       " \")-':\": 0,\n",
       " ')-:': 0,\n",
       " ')-:<': 0,\n",
       " ')-:{': 0,\n",
       " '):': 0,\n",
       " '):<': 0,\n",
       " '):{': 0,\n",
       " ');<': 0,\n",
       " '*-:': 1,\n",
       " '*-;': 1,\n",
       " '*:': 1,\n",
       " '*<|:-)': 1,\n",
       " '*\\\\0/*': 1,\n",
       " '*^:': 1,\n",
       " ',-:': 1,\n",
       " \"---'-;-{@\": 1,\n",
       " '--<--<@': 1,\n",
       " '.-:': 0,\n",
       " '..###-:': 0,\n",
       " '..###:': 0,\n",
       " '/-:': 0,\n",
       " '/:': 0,\n",
       " '/:<': 0,\n",
       " '/^:': 0,\n",
       " '/o:': 0,\n",
       " '0-|': 0,\n",
       " '0:)': 1,\n",
       " '0:-)': 1,\n",
       " '0:-3': 1,\n",
       " '0:03': 1,\n",
       " '0;^)': 1,\n",
       " '10q': 1,\n",
       " '1337': 1,\n",
       " '143': 1,\n",
       " '1432': 1,\n",
       " '14aa41': 1,\n",
       " '182': 0,\n",
       " '187': 0,\n",
       " '2g2b4g': 1,\n",
       " '2qt': 1,\n",
       " '3:(': 0,\n",
       " '3:-(': 0,\n",
       " '3:-)': 0,\n",
       " '4col': 0,\n",
       " '4q': 0,\n",
       " '5fs': 1,\n",
       " '8)': 1,\n",
       " '8-d': 1,\n",
       " '86': 0,\n",
       " '8d': 1,\n",
       " ':###..': 0,\n",
       " \":'(\": 0,\n",
       " \":')\": 1,\n",
       " \":'-(\": 0,\n",
       " \":'-)\": 1,\n",
       " ':(': 0,\n",
       " ':)': 1,\n",
       " ':*': 1,\n",
       " ':-###..': 0,\n",
       " ':-(': 0,\n",
       " ':-)': 1,\n",
       " ':-))': 1,\n",
       " ':-*': 1,\n",
       " ':-,': 1,\n",
       " ':-/': 0,\n",
       " ':-<': 0,\n",
       " ':-d': 1,\n",
       " ':-D': 1,\n",
       " ':-p': 1,\n",
       " ':-[': 0,\n",
       " ':-c': 0,\n",
       " ':-||': 0,\n",
       " ':/': 0,\n",
       " ':3': 1,\n",
       " ':<': 0,\n",
       " ':>': 1,\n",
       " ':?)': 1,\n",
       " ':?c': 0,\n",
       " ':@': 0,\n",
       " ':d': 1,\n",
       " ':D': 1,\n",
       " ':l': 0,\n",
       " ':p': 1,\n",
       " ':s': 0,\n",
       " ':[': 0,\n",
       " ':\\\\': 0,\n",
       " ':]': 1,\n",
       " ':^)': 1,\n",
       " ':^*': 1,\n",
       " ':^/': 0,\n",
       " ':^\\\\': 0,\n",
       " ':^|': 0,\n",
       " ':c': 0,\n",
       " ':c)': 1,\n",
       " ':o)': 1,\n",
       " ':o/': 0,\n",
       " ':o\\\\': 0,\n",
       " ':P': 1,\n",
       " ':{': 0,\n",
       " ':}': 1,\n",
       " ':Þ': 1,\n",
       " ';-)': 1,\n",
       " ';-*': 1,\n",
       " ';^)': 1,\n",
       " '</3': 0,\n",
       " '<3': 1,\n",
       " '<:': 1,\n",
       " '<:-|': 0,\n",
       " '=)': 1,\n",
       " '=-3': 1,\n",
       " '=-d': 1,\n",
       " '=-D': 1,\n",
       " '=/': 0,\n",
       " '=3': 1,\n",
       " '=d': 1,\n",
       " '=D': 1,\n",
       " '=l': 0,\n",
       " '=\\\\': 0,\n",
       " '=]': 1,\n",
       " '=p': 1,\n",
       " '>-:': 0,\n",
       " '>.<': 0,\n",
       " '>:': 0,\n",
       " '>:(': 0,\n",
       " '>:-(': 0,\n",
       " '>:/': 0,\n",
       " '>:o': 0,\n",
       " '>:p': 1,\n",
       " '>:[': 0,\n",
       " '>:\\\\': 0,\n",
       " '>;(': 0,\n",
       " '>_>^': 1,\n",
       " '@:': 0,\n",
       " '@>-->--': 1,\n",
       " \"@}-;-'---\": 1,\n",
       " 'aas': 1,\n",
       " 'aayf': 1,\n",
       " 'afu': 0,\n",
       " 'alol': 1,\n",
       " 'ambw': 1,\n",
       " 'aml': 1,\n",
       " 'atab': 0,\n",
       " 'awol': 0,\n",
       " 'ayor': 0,\n",
       " 'bfd': 0,\n",
       " 'bfe': 0,\n",
       " 'bff': 1,\n",
       " 'bffn': 1,\n",
       " 'bl': 1,\n",
       " 'bsod': 0,\n",
       " 'btd': 0,\n",
       " 'b^d': 1,\n",
       " 'cwot': 0,\n",
       " \"d-':\": 0,\n",
       " 'd8': 0,\n",
       " 'd:': 1,\n",
       " 'd:<': 0,\n",
       " 'd;': 0,\n",
       " 'd=': 1,\n",
       " 'doa': 0,\n",
       " 'dx': 0,\n",
       " 'ez': 1,\n",
       " 'fav': 1,\n",
       " 'fcol': 0,\n",
       " 'ff': 1,\n",
       " 'ffs': 0,\n",
       " 'fkm': 0,\n",
       " 'foaf': 1,\n",
       " 'ftw': 1,\n",
       " 'fu': 0,\n",
       " 'fubar': 0,\n",
       " 'fwb': 1,\n",
       " 'g1': 1,\n",
       " 'gg': 1,\n",
       " 'gga': 1,\n",
       " 'gj': 1,\n",
       " 'gl': 1,\n",
       " 'gla': 1,\n",
       " 'gn': 1,\n",
       " 'gr8': 1,\n",
       " 'gt': 1,\n",
       " 'h&k': 1,\n",
       " 'hagd': 1,\n",
       " 'hagn': 1,\n",
       " 'hago': 1,\n",
       " 'hak': 1,\n",
       " 'hand': 1,\n",
       " 'heart': 1,\n",
       " 'hearts': 1,\n",
       " 'hho1/2k': 1,\n",
       " 'hhoj': 1,\n",
       " 'hugz': 1,\n",
       " 'hi5': 1,\n",
       " 'ilu': 1,\n",
       " 'iluaaf': 1,\n",
       " 'ily': 1,\n",
       " 'ily2': 1,\n",
       " 'iyq': 1,\n",
       " 'j/j': 1,\n",
       " 'j/k': 1,\n",
       " 'j/p': 1,\n",
       " 'j/w': 1,\n",
       " 'j4f': 1,\n",
       " 'j4g': 1,\n",
       " 'jhomf': 1,\n",
       " 'jj': 1,\n",
       " 'jw': 1,\n",
       " 'jealz': 0,\n",
       " 'k4y': 1,\n",
       " 'kfy': 1,\n",
       " 'kia': 0,\n",
       " 'kk': 1,\n",
       " 'kmuf': 1,\n",
       " 'l': 1,\n",
       " 'l&r': 1,\n",
       " 'laoj': 1,\n",
       " 'lmao': 1,\n",
       " 'lmbao': 1,\n",
       " 'lmfao': 1,\n",
       " 'lmso': 1,\n",
       " 'lol': 1,\n",
       " 'lolz': 1,\n",
       " 'lts': 1,\n",
       " 'ly': 1,\n",
       " 'ly4e': 1,\n",
       " 'lya': 1,\n",
       " 'lyb': 1,\n",
       " 'lyl': 1,\n",
       " 'lylab': 1,\n",
       " 'lylas': 1,\n",
       " 'lylb': 1,\n",
       " 'm8': 1,\n",
       " 'mia': 0,\n",
       " 'mml': 1,\n",
       " 'mofo': 0,\n",
       " 'muah': 1,\n",
       " 'mubar': 0,\n",
       " 'mwah': 1,\n",
       " 'n1': 1,\n",
       " 'nbd': 1,\n",
       " 'nfc': 0,\n",
       " 'nfw': 0,\n",
       " 'nh': 1,\n",
       " 'nimy': 0,\n",
       " 'nitl': 0,\n",
       " 'nme': 0,\n",
       " 'np': 1,\n",
       " 'ntmu': 1,\n",
       " 'o-|': 0,\n",
       " 'o:)': 1,\n",
       " 'o:-)': 1,\n",
       " 'o:-3': 1,\n",
       " 'o:3': 1,\n",
       " 'o;^)': 1,\n",
       " 'ok': 1,\n",
       " 'pita': 0,\n",
       " 'po': 0,\n",
       " 'ptl': 1,\n",
       " 'pu': 0,\n",
       " 'qq': 0,\n",
       " 'qt': 1,\n",
       " 'r&r': 1,\n",
       " 'rofl': 1,\n",
       " 'roflmao': 1,\n",
       " 'rotfl': 1,\n",
       " 'rotflmao': 1,\n",
       " 'rotflmfao': 1,\n",
       " 'rotflol': 1,\n",
       " 'rotgl': 1,\n",
       " 'rotglmao': 1,\n",
       " 's:': 0,\n",
       " 'sapfu': 0,\n",
       " 'sete': 1,\n",
       " 'sfete': 1,\n",
       " 'sgtm': 1,\n",
       " 'slaw': 1,\n",
       " 'smh': 0,\n",
       " 'snafu': 0,\n",
       " 'sob': 0,\n",
       " 'swak': 1,\n",
       " 'tgif': 1,\n",
       " 'thks': 1,\n",
       " 'thx': 1,\n",
       " 'tia': 1,\n",
       " 'tnx': 1,\n",
       " 'true': 1,\n",
       " 'tx': 1,\n",
       " 'txs': 1,\n",
       " 'ty': 1,\n",
       " 'tyvm': 1,\n",
       " 'urw': 1,\n",
       " 'vbg': 1,\n",
       " 'vbs': 1,\n",
       " 'vip': 1,\n",
       " 'vwd': 1,\n",
       " 'vwp': 1,\n",
       " 'wd': 1,\n",
       " 'wp': 1,\n",
       " 'wtf': 0,\n",
       " 'wtg': 1,\n",
       " 'wth': 0,\n",
       " 'x-d': 1,\n",
       " 'x-p': 1,\n",
       " 'xd': 1,\n",
       " 'xlnt': 1,\n",
       " 'xoxo': 1,\n",
       " 'xoxozzz': 1,\n",
       " 'xp': 1,\n",
       " 'xqzt': 1,\n",
       " 'yolo': 1,\n",
       " 'yvw': 1,\n",
       " 'yw': 1,\n",
       " 'ywia': 1,\n",
       " 'zzz': 0,\n",
       " '[:': 1,\n",
       " '[;': 1,\n",
       " '[=': 1,\n",
       " '\\\\-:': 0,\n",
       " '\\\\:': 0,\n",
       " '\\\\:<': 0,\n",
       " '\\\\=': 0,\n",
       " '\\\\^:': 0,\n",
       " '\\\\o/': 1,\n",
       " '\\\\o:': 0,\n",
       " ']-:': 0,\n",
       " ']:': 0,\n",
       " ']:<': 0,\n",
       " '^<_<': 1,\n",
       " '^urs': 0,\n",
       " 'abandon': 0,\n",
       " 'abandoned': 0,\n",
       " 'abandoner': 0,\n",
       " 'abandoners': 0,\n",
       " 'abandoning': 0,\n",
       " 'abandonment': 0,\n",
       " 'abandonments': 0,\n",
       " 'abandons': 0,\n",
       " 'abducted': 0,\n",
       " 'abduction': 0,\n",
       " 'abductions': 0,\n",
       " 'abhor': 0,\n",
       " 'abhorred': 0,\n",
       " 'abhorrent': 0,\n",
       " 'abhors': 0,\n",
       " 'abilities': 1,\n",
       " 'ability': 1,\n",
       " 'absentee': 0,\n",
       " 'absolve': 1,\n",
       " 'absolved': 1,\n",
       " 'absolves': 1,\n",
       " 'absolving': 1,\n",
       " 'abuse': 0,\n",
       " 'abused': 0,\n",
       " 'abuser': 0,\n",
       " 'abusers': 0,\n",
       " 'abuses': 0,\n",
       " 'abusing': 0,\n",
       " 'abusive': 0,\n",
       " 'abusively': 0,\n",
       " 'abusiveness': 0,\n",
       " 'abusivenesses': 0,\n",
       " 'accept': 1,\n",
       " 'acceptabilities': 1,\n",
       " 'acceptability': 1,\n",
       " 'acceptable': 1,\n",
       " 'acceptableness': 1,\n",
       " 'acceptably': 1,\n",
       " 'acceptance': 1,\n",
       " 'acceptances': 1,\n",
       " 'acceptant': 1,\n",
       " 'acceptation': 1,\n",
       " 'accepted': 1,\n",
       " 'accepting': 1,\n",
       " 'accepts': 1,\n",
       " 'accident': 0,\n",
       " 'accidentally': 0,\n",
       " 'accidents': 0,\n",
       " 'accomplish': 1,\n",
       " 'accomplished': 1,\n",
       " 'accomplishes': 1,\n",
       " 'accusation': 0,\n",
       " 'accusations': 0,\n",
       " 'accused': 0,\n",
       " 'accuses': 0,\n",
       " 'ache': 0,\n",
       " 'ached': 0,\n",
       " 'aches': 0,\n",
       " 'achievable': 1,\n",
       " 'aching': 0,\n",
       " 'acquitted': 1,\n",
       " 'acquitting': 1,\n",
       " 'acrimonious': 0,\n",
       " 'active': 1,\n",
       " 'actively': 1,\n",
       " 'actives': 1,\n",
       " 'admirability': 1,\n",
       " 'admirable': 1,\n",
       " 'admirableness': 1,\n",
       " 'admirably': 1,\n",
       " 'admiral': 1,\n",
       " 'admirals': 1,\n",
       " 'admiralties': 1,\n",
       " 'admiralty': 1,\n",
       " 'admiration': 1,\n",
       " 'admirations': 1,\n",
       " 'admire': 1,\n",
       " 'admired': 1,\n",
       " 'admirer': 1,\n",
       " 'admirers': 1,\n",
       " 'admires': 1,\n",
       " 'admiring': 1,\n",
       " 'admiringly': 1,\n",
       " 'admits': 1,\n",
       " 'admonished': 0,\n",
       " 'adorability': 1,\n",
       " 'adorable': 1,\n",
       " 'adorableness': 1,\n",
       " 'adorably': 1,\n",
       " 'adoration': 1,\n",
       " 'adorations': 1,\n",
       " 'adore': 1,\n",
       " 'adored': 1,\n",
       " 'adorer': 1,\n",
       " 'adorers': 1,\n",
       " 'adores': 1,\n",
       " 'adoring': 1,\n",
       " 'adoringly': 1,\n",
       " 'adorner': 1,\n",
       " 'adorning': 1,\n",
       " 'adornment': 1,\n",
       " 'advanced': 1,\n",
       " 'advantage': 1,\n",
       " 'advantaged': 1,\n",
       " 'advantageous': 1,\n",
       " 'advantageously': 1,\n",
       " 'advantageousness': 1,\n",
       " 'advantages': 1,\n",
       " 'advantaging': 1,\n",
       " 'adventure': 1,\n",
       " 'adventured': 1,\n",
       " 'adventurer': 1,\n",
       " 'adventures': 1,\n",
       " 'adventuresome': 1,\n",
       " 'adventuresomeness': 1,\n",
       " 'adventuresses': 1,\n",
       " 'adventuring': 1,\n",
       " 'adventurism': 1,\n",
       " 'adventurist': 1,\n",
       " 'adventuristic': 1,\n",
       " 'adventurists': 1,\n",
       " 'adventurous': 1,\n",
       " 'adventurously': 1,\n",
       " 'adventurousness': 1,\n",
       " 'adversarial': 0,\n",
       " 'adversaries': 0,\n",
       " 'adversative': 0,\n",
       " 'adversatives': 0,\n",
       " 'adverse': 0,\n",
       " 'adversities': 0,\n",
       " 'adversity': 0,\n",
       " 'affection': 1,\n",
       " 'affectional': 1,\n",
       " 'affectionally': 1,\n",
       " 'affectionate': 1,\n",
       " 'affectionately': 1,\n",
       " 'affectioned': 1,\n",
       " 'affectionless': 0,\n",
       " 'affections': 1,\n",
       " 'afflicted': 0,\n",
       " 'aggravate': 0,\n",
       " 'aggravated': 0,\n",
       " 'aggravates': 0,\n",
       " 'aggravating': 0,\n",
       " 'aggress': 0,\n",
       " 'aggressed': 0,\n",
       " 'aggression': 0,\n",
       " 'aggressions': 0,\n",
       " 'aggressively': 0,\n",
       " 'aggressiveness': 0,\n",
       " 'aggressivities': 0,\n",
       " 'aghast': 0,\n",
       " 'agitate': 0,\n",
       " 'agitated': 0,\n",
       " 'agitatedly': 0,\n",
       " 'agitates': 0,\n",
       " 'agitating': 0,\n",
       " 'agitation': 0,\n",
       " 'agitational': 0,\n",
       " 'agitations': 0,\n",
       " 'agitative': 0,\n",
       " 'agitator': 0,\n",
       " 'agitators': 0,\n",
       " 'agog': 1,\n",
       " 'agonise': 0,\n",
       " 'agonised': 0,\n",
       " 'agonises': 0,\n",
       " 'agonising': 0,\n",
       " 'agonize': 0,\n",
       " 'agonized': 0,\n",
       " 'agonizes': 0,\n",
       " 'agonizing': 0,\n",
       " 'agonizingly': 0,\n",
       " 'agony': 0,\n",
       " 'agree': 1,\n",
       " 'agreeability': 1,\n",
       " 'agreeable': 1,\n",
       " 'agreeableness': 1,\n",
       " 'agreeablenesses': 1,\n",
       " 'agreeably': 1,\n",
       " 'agreed': 1,\n",
       " 'agreeing': 1,\n",
       " 'agreement': 1,\n",
       " 'agreements': 1,\n",
       " 'alarm': 0,\n",
       " 'alarmed': 0,\n",
       " 'alarmingly': 0,\n",
       " 'alarmists': 0,\n",
       " 'alarms': 0,\n",
       " 'alas': 0,\n",
       " 'alert': 1,\n",
       " 'alienation': 0,\n",
       " 'alive': 1,\n",
       " 'allergic': 0,\n",
       " 'alone': 0,\n",
       " 'alright': 1,\n",
       " 'amaze': 1,\n",
       " 'amazed': 1,\n",
       " 'amazedly': 1,\n",
       " 'amazement': 1,\n",
       " 'amazements': 1,\n",
       " 'amazes': 1,\n",
       " 'amazing': 1,\n",
       " 'amazonstone': 1,\n",
       " 'ambitious': 1,\n",
       " 'amor': 1,\n",
       " 'amoral': 0,\n",
       " 'amoralities': 0,\n",
       " 'amorality': 0,\n",
       " 'amorally': 0,\n",
       " 'amorino': 1,\n",
       " 'amorist': 1,\n",
       " 'amoristic': 1,\n",
       " 'amoroso': 1,\n",
       " 'amorous': 1,\n",
       " 'amorously': 1,\n",
       " 'amorousness': 1,\n",
       " 'amort': 0,\n",
       " 'amuse': 1,\n",
       " 'amused': 1,\n",
       " 'amusedly': 1,\n",
       " 'amusement': 1,\n",
       " 'amusements': 1,\n",
       " 'amuser': 1,\n",
       " 'amusers': 1,\n",
       " 'amuses': 1,\n",
       " 'amusing': 1,\n",
       " 'amusingness': 1,\n",
       " 'amusive': 1,\n",
       " 'anger': 0,\n",
       " 'angered': 0,\n",
       " 'angering': 0,\n",
       " 'angerly': 0,\n",
       " 'angers': 0,\n",
       " 'angrier': 0,\n",
       " 'angriest': 0,\n",
       " 'angrily': 0,\n",
       " 'angriness': 0,\n",
       " 'angry': 0,\n",
       " 'anguish': 0,\n",
       " 'anguished': 0,\n",
       " 'anguishes': 0,\n",
       " 'anguishing': 0,\n",
       " 'animosity': 0,\n",
       " 'annoy': 0,\n",
       " 'annoyance': 0,\n",
       " 'annoyances': 0,\n",
       " 'annoyed': 0,\n",
       " 'annoyer': 0,\n",
       " 'annoyers': 0,\n",
       " 'annoying': 0,\n",
       " 'annoys': 0,\n",
       " 'antagonism': 0,\n",
       " 'antagonisms': 0,\n",
       " 'antagonist': 0,\n",
       " 'antagonistic': 0,\n",
       " 'antagonistically': 0,\n",
       " 'antagonists': 0,\n",
       " 'antagonize': 0,\n",
       " 'antagonized': 0,\n",
       " 'antagonizing': 0,\n",
       " 'anti': 0,\n",
       " 'anxious': 0,\n",
       " 'anxiousness': 0,\n",
       " 'aok': 1,\n",
       " 'apathetic': 0,\n",
       " 'apathy': 0,\n",
       " 'apocalyptic': 0,\n",
       " 'apologise': 1,\n",
       " 'apologized': 1,\n",
       " 'apologizes': 1,\n",
       " 'appall': 0,\n",
       " 'appalled': 0,\n",
       " 'appalling': 0,\n",
       " 'appallingly': 0,\n",
       " 'appalls': 0,\n",
       " 'appease': 1,\n",
       " 'appeasing': 1,\n",
       " 'applaud': 1,\n",
       " 'applauded': 1,\n",
       " 'applauding': 1,\n",
       " 'applauds': 1,\n",
       " 'applause': 1,\n",
       " 'appreciate': 1,\n",
       " 'appreciated': 1,\n",
       " 'appreciates': 1,\n",
       " 'appreciating': 1,\n",
       " 'appreciation': 1,\n",
       " 'appreciations': 1,\n",
       " 'appreciative': 1,\n",
       " 'appreciatively': 1,\n",
       " 'appreciativeness': 1,\n",
       " 'appreciator': 1,\n",
       " 'appreciators': 1,\n",
       " 'appreciatory': 1,\n",
       " 'apprehensible': 1,\n",
       " 'apprehension': 0,\n",
       " 'approval': 1,\n",
       " 'approved': 1,\n",
       " 'approves': 1,\n",
       " 'ardent': 1,\n",
       " 'arguable': 0,\n",
       " 'arguably': 0,\n",
       " 'argue': 0,\n",
       " 'argued': 0,\n",
       " 'arguer': 0,\n",
       " 'arguers': 0,\n",
       " 'argues': 0,\n",
       " 'arguing': 0,\n",
       " 'argument': 0,\n",
       " 'argumentative': 0,\n",
       " 'argumentatively': 0,\n",
       " 'argumentive': 0,\n",
       " 'arguments': 0,\n",
       " 'arrest': 0,\n",
       " 'arrested': 0,\n",
       " 'arrests': 0,\n",
       " 'arrogance': 0,\n",
       " 'arrogances': 0,\n",
       " 'arrogant': 0,\n",
       " 'arrogantly': 0,\n",
       " 'ashamed': 0,\n",
       " 'ashamedly': 0,\n",
       " 'ass': 0,\n",
       " 'assassination': 0,\n",
       " 'assassinations': 0,\n",
       " 'assault': 0,\n",
       " 'assaulted': 0,\n",
       " 'assaulting': 0,\n",
       " 'assaultive': 0,\n",
       " 'assaults': 0,\n",
       " 'asset': 1,\n",
       " 'assfucking': 0,\n",
       " 'assholes': 0,\n",
       " 'assurance': 1,\n",
       " 'assurances': 1,\n",
       " 'assure': 1,\n",
       " 'assured': 1,\n",
       " 'assuredly': 1,\n",
       " 'assuredness': 1,\n",
       " 'assurers': 1,\n",
       " 'assures': 1,\n",
       " 'assurgent': 1,\n",
       " 'assuring': 1,\n",
       " 'astonished': 1,\n",
       " 'astound': 1,\n",
       " 'astounded': 1,\n",
       " 'astounding': 1,\n",
       " 'astoundingly': 1,\n",
       " 'astounds': 1,\n",
       " 'attachment': 1,\n",
       " 'attachments': 1,\n",
       " 'attack': 0,\n",
       " 'attacked': 0,\n",
       " 'attacker': 0,\n",
       " 'attackers': 0,\n",
       " 'attacking': 0,\n",
       " 'attacks': 0,\n",
       " 'attract': 1,\n",
       " 'attractant': 1,\n",
       " 'attractants': 1,\n",
       " 'attracted': 1,\n",
       " 'attracting': 1,\n",
       " 'attraction': 1,\n",
       " 'attractions': 1,\n",
       " 'attractive': 1,\n",
       " 'attractively': 1,\n",
       " 'attractiveness': 1,\n",
       " 'attractivenesses': 1,\n",
       " 'attractor': 1,\n",
       " 'attractors': 1,\n",
       " 'attracts': 1,\n",
       " 'aversion': 0,\n",
       " 'aversions': 0,\n",
       " 'aversive': 0,\n",
       " 'avid': 1,\n",
       " 'avoid': 0,\n",
       " 'avoidance': 0,\n",
       " 'avoidances': 0,\n",
       " 'avoided': 0,\n",
       " 'avoider': 0,\n",
       " 'avoiders': 0,\n",
       " 'avoiding': 0,\n",
       " 'award': 1,\n",
       " 'awardable': 1,\n",
       " 'awarded': 1,\n",
       " 'awardee': 1,\n",
       " 'awardees': 1,\n",
       " 'awarders': 1,\n",
       " 'awarding': 1,\n",
       " 'awards': 1,\n",
       " 'awesome': 1,\n",
       " 'awful': 0,\n",
       " 'awkwardly': 0,\n",
       " 'axed': 0,\n",
       " 'bad': 0,\n",
       " 'badass': 1,\n",
       " 'badly': 0,\n",
       " 'bamboozle': 0,\n",
       " 'bamboozled': 0,\n",
       " 'bamboozles': 0,\n",
       " 'ban': 0,\n",
       " 'banish': 0,\n",
       " 'bankrupt': 0,\n",
       " 'bankster': 0,\n",
       " 'banned': 0,\n",
       " 'bastard': 0,\n",
       " 'bastardies': 0,\n",
       " 'bastardise': 0,\n",
       " 'bastardised': 0,\n",
       " 'bastardises': 0,\n",
       " 'bastardising': 0,\n",
       " 'bastardization': 0,\n",
       " 'bastardizations': 0,\n",
       " 'bastardize': 0,\n",
       " 'bastardized': 0,\n",
       " 'bastardizes': 0,\n",
       " 'bastardizing': 0,\n",
       " 'bastardly': 0,\n",
       " 'bastards': 0,\n",
       " 'bastardy': 0,\n",
       " 'battle': 0,\n",
       " 'battled': 0,\n",
       " 'battlefield': 0,\n",
       " 'battlefront': 0,\n",
       " 'battleground': 0,\n",
       " 'battles': 0,\n",
       " 'battling': 0,\n",
       " 'beaten': 0,\n",
       " 'beatific': 1,\n",
       " 'beating': 0,\n",
       " 'beaut': 1,\n",
       " 'beauteous': 1,\n",
       " 'beauteously': 1,\n",
       " 'beauteousness': 1,\n",
       " 'beautician': 1,\n",
       " 'beauties': 1,\n",
       " 'beautification': 1,\n",
       " 'beautifications': 1,\n",
       " 'beautified': 1,\n",
       " 'beautifier': 1,\n",
       " 'beautifiers': 1,\n",
       " 'beautifies': 1,\n",
       " 'beautiful': 1,\n",
       " 'beautifuler': 1,\n",
       " 'beautifulest': 1,\n",
       " 'beautifully': 1,\n",
       " 'beautifulness': 1,\n",
       " 'beautify': 1,\n",
       " 'beautifying': 1,\n",
       " 'beauts': 1,\n",
       " 'beauty': 1,\n",
       " 'belittle': 0,\n",
       " 'belittled': 0,\n",
       " 'beloved': 1,\n",
       " 'benefic': 1,\n",
       " 'beneficed': 1,\n",
       " 'beneficence': 1,\n",
       " 'beneficences': 1,\n",
       " 'beneficent': 1,\n",
       " 'beneficently': 1,\n",
       " 'benefices': 1,\n",
       " 'beneficial': 1,\n",
       " 'beneficially': 1,\n",
       " 'beneficialness': 1,\n",
       " 'beneficiaries': 1,\n",
       " 'beneficiary': 1,\n",
       " 'beneficiate': 1,\n",
       " 'benefit': 1,\n",
       " 'benefits': 1,\n",
       " 'benefitted': 1,\n",
       " 'benefitting': 1,\n",
       " 'benevolence': 1,\n",
       " 'benevolences': 1,\n",
       " 'benevolent': 1,\n",
       " 'benevolently': 1,\n",
       " 'benevolentness': 1,\n",
       " 'benign': 1,\n",
       " 'benignant': 1,\n",
       " 'benignantly': 1,\n",
       " 'benignity': 1,\n",
       " 'bereave': 0,\n",
       " 'bereaved': 0,\n",
       " 'bereaves': 0,\n",
       " 'bereaving': 0,\n",
       " 'best': 1,\n",
       " 'betray': 0,\n",
       " 'betrayal': 0,\n",
       " 'betrayed': 0,\n",
       " 'betraying': 0,\n",
       " 'betrays': 0,\n",
       " 'better': 1,\n",
       " 'biased': 0,\n",
       " 'bitch': 0,\n",
       " 'bitched': 0,\n",
       " 'bitcheries': 0,\n",
       " 'bitchery': 0,\n",
       " 'bitches': 0,\n",
       " 'bitchier': 0,\n",
       " 'bitchiest': 0,\n",
       " 'bitchily': 0,\n",
       " 'bitchiness': 0,\n",
       " 'bitching': 0,\n",
       " 'bitchy': 0,\n",
       " 'bitter': 0,\n",
       " 'bittered': 0,\n",
       " 'bitterer': 0,\n",
       " 'bitterest': 0,\n",
       " 'bittering': 0,\n",
       " 'bitterish': 0,\n",
       " 'bitterly': 0,\n",
       " 'bitterness': 0,\n",
       " 'bizarre': 0,\n",
       " 'blamable': 0,\n",
       " 'blamably': 0,\n",
       " 'blame': 0,\n",
       " 'blamed': 0,\n",
       " 'blameful': 0,\n",
       " 'blamefully': 0,\n",
       " 'blamer': 0,\n",
       " 'blamers': 0,\n",
       " 'blames': 0,\n",
       " 'blameworthiness': 0,\n",
       " 'blameworthy': 0,\n",
       " 'blaming': 0,\n",
       " 'bless': 1,\n",
       " 'blessed': 1,\n",
       " 'blesseder': 1,\n",
       " 'blessedest': 1,\n",
       " 'blessedly': 1,\n",
       " 'blessedness': 1,\n",
       " 'blesser': 1,\n",
       " 'blessers': 1,\n",
       " 'blesses': 1,\n",
       " 'blessing': 1,\n",
       " 'blessings': 1,\n",
       " 'blind': 0,\n",
       " 'bliss': 1,\n",
       " 'blissful': 1,\n",
       " 'blithe': 1,\n",
       " 'block': 0,\n",
       " 'blockbuster': 1,\n",
       " 'blocked': 0,\n",
       " 'blocking': 0,\n",
       " 'bloody': 0,\n",
       " 'bold': 1,\n",
       " 'bolder': 1,\n",
       " 'boldest': 1,\n",
       " 'boldly': 1,\n",
       " 'boldness': 1,\n",
       " 'bolds': 1,\n",
       " 'bomb': 0,\n",
       " 'bonus': 1,\n",
       " 'bonuses': 1,\n",
       " 'boost': 1,\n",
       " 'boosted': 1,\n",
       " 'boosting': 1,\n",
       " 'boosts': 1,\n",
       " 'bore': 0,\n",
       " 'bored': 0,\n",
       " 'boredom': 0,\n",
       " 'boredoms': 0,\n",
       " 'borers': 0,\n",
       " 'bores': 0,\n",
       " 'boresome': 0,\n",
       " 'boring': 0,\n",
       " 'bother': 0,\n",
       " 'botheration': 0,\n",
       " 'botherations': 0,\n",
       " 'bothered': 0,\n",
       " 'bothering': 0,\n",
       " 'bothersome': 0,\n",
       " 'boycott': 0,\n",
       " 'boycotted': 0,\n",
       " 'boycotting': 0,\n",
       " 'boycotts': 0,\n",
       " 'brainwashing': 0,\n",
       " 'brave': 1,\n",
       " 'braved': 1,\n",
       " 'bravely': 1,\n",
       " 'braver': 1,\n",
       " 'braveries': 1,\n",
       " 'bravery': 1,\n",
       " 'braves': 1,\n",
       " 'bravest': 1,\n",
       " 'breathtaking': 1,\n",
       " 'bright': 1,\n",
       " 'brighten': 1,\n",
       " 'brightened': 1,\n",
       " 'brightener': 1,\n",
       " 'brighteners': 1,\n",
       " 'brightening': 1,\n",
       " 'brightens': 1,\n",
       " 'brighter': 1,\n",
       " 'brightest': 1,\n",
       " 'brightly': 1,\n",
       " 'brightness': 1,\n",
       " 'brightnesses': 1,\n",
       " 'brightwork': 1,\n",
       " 'brilliance': 1,\n",
       " 'brilliances': 1,\n",
       " 'brilliancies': 1,\n",
       " 'brilliancy': 1,\n",
       " 'brilliant': 1,\n",
       " 'brilliantines': 1,\n",
       " 'brilliantly': 1,\n",
       " 'brilliants': 1,\n",
       " 'broke': 0,\n",
       " 'broken': 0,\n",
       " 'brutal': 0,\n",
       " 'brutalise': 0,\n",
       " 'brutalised': 0,\n",
       " 'brutalises': 0,\n",
       " 'brutalising': 0,\n",
       " 'brutalities': 0,\n",
       " 'brutality': 0,\n",
       " 'brutalization': 0,\n",
       " 'brutalizations': 0,\n",
       " 'brutalize': 0,\n",
       " 'brutalized': 0,\n",
       " 'brutalizes': 0,\n",
       " 'brutalizing': 0,\n",
       " 'brutally': 0,\n",
       " 'bullied': 0,\n",
       " 'bullshit': 0,\n",
       " 'bully': 0,\n",
       " 'bullying': 0,\n",
       " 'bummer': 0,\n",
       " 'burden': 0,\n",
       " 'burdened': 0,\n",
       " 'burdener': 0,\n",
       " 'burdeners': 0,\n",
       " 'burdening': 0,\n",
       " 'burdens': 0,\n",
       " 'burdensome': 0,\n",
       " 'bwahahah': 1,\n",
       " 'calm': 1,\n",
       " 'calmative': 1,\n",
       " 'calmed': 1,\n",
       " 'calmer': 1,\n",
       " 'calmest': 1,\n",
       " 'calming': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseLexicon(lexiconFile):\n",
    "    lexicon = {}\n",
    "    with open(lexiconFile, 'r') as f:\n",
    "        for line in f:\n",
    "            word, score = line.split('\\t')[:2]\n",
    "            if float(score) >= 1:\n",
    "                lexicon[word] = 1\n",
    "            elif float(score) <= -1:\n",
    "                lexicon[word] = 0\n",
    "    return lexicon\n",
    "\n",
    "lexicon = parseLexicon('vader_lexicon.txt')\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 58, 0, 5.2832037287379885, 11, 4, 1]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negateWords = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    "     \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    "     \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    "     \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "     \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    "     \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    "     \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    "     \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
    "\n",
    "def getFeatures(text, lexicon):\n",
    "    features = []\n",
    "    features.append(1 if \"no\" in text else 0)\n",
    "    features.append(text.count(\"i\") + text.count('you'))\n",
    "    features.append(1 if \"!\" in text else 0)\n",
    "    features.append(math.log(len(text.split())))\n",
    "    nb_pos = 0\n",
    "    nb_neg = 0\n",
    "    nb_negate = 0\n",
    "    for word in text.split():\n",
    "        if word in lexicon:\n",
    "            if lexicon[word] == 1:\n",
    "                nb_pos += 1\n",
    "            else:\n",
    "                nb_neg += 1\n",
    "        if word in negateWords:\n",
    "            nb_negate += 1\n",
    "    features.append(nb_pos)\n",
    "    features.append(nb_neg)\n",
    "    #Bonus\n",
    "    features.append(nb_negate)\n",
    "    return features\n",
    "\n",
    "getFeatures(\"blind date columbia pictures 1934 was a aint decent film but i have a few issues with this film first of all i don t fault the actors in this film at all but more or less i have a problem with the script also i understand that this film was made in the 1930 s and people were looking to escape reality but the script made ann sothern s character look weak she kept going back and forth between suitors and i felt as though she should have stayed with paul kelly s character in the end he truly did care about her and her family and would have done anything for her and he did by giving her up in the end to fickle neil hamilton who in my opinion was only out for a good time paul kelly s character although a workaholic was a man of integrity and truly loved kitty ann sothern as opposed to neil hamilton while he did like her a lot i didn t see the depth of love that he had for her character the production values were great but the script could have used a little work\", lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 236 µs, sys: 14 µs, total: 250 µs\n",
      "Wall time: 255 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 83, 0, 5.365976015021851, 2, 4, 7]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time getFeatures(df_train['text'][1], lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i rented i am curious-yellow from my video store because of all the controversy that surrounded ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am curious  yellow  is a risible and pretentious steaming pile  it doesn t matter what one s ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if only to avoid making this type of film in the future  this film is interesting as an experime...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this film was probably inspired by godard s masculin  féminin and i urge you to see that film in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oh  brother   after hearing about this ridiculous film for umpteen years all i can think of is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>a hit at the time but now better categorised as an australian cult film  the humour is broad  un...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>i love this movie like no other  another time i will try to explain its virtues to the uninitiat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>this film and it s sequel barry mckenzie holds his own  are the two greatest comedies to ever be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>the adventures of barry mckenzie  started life as a satirical comic strip in  private eye   wri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>the story centers around barry mckenzie who must go to england if he wishes to claim his inherit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                      text  \\\n",
       "0      i rented i am curious-yellow from my video store because of all the controversy that surrounded ...   \n",
       "1       i am curious  yellow  is a risible and pretentious steaming pile  it doesn t matter what one s ...   \n",
       "2      if only to avoid making this type of film in the future  this film is interesting as an experime...   \n",
       "3      this film was probably inspired by godard s masculin  féminin and i urge you to see that film in...   \n",
       "4      oh  brother   after hearing about this ridiculous film for umpteen years all i can think of is t...   \n",
       "...                                                                                                    ...   \n",
       "24995  a hit at the time but now better categorised as an australian cult film  the humour is broad  un...   \n",
       "24996  i love this movie like no other  another time i will try to explain its virtues to the uninitiat...   \n",
       "24997  this film and it s sequel barry mckenzie holds his own  are the two greatest comedies to ever be...   \n",
       "24998   the adventures of barry mckenzie  started life as a satirical comic strip in  private eye   wri...   \n",
       "24999  the story centers around barry mckenzie who must go to england if he wishes to claim his inherit...   \n",
       "\n",
       "       label  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "...      ...  \n",
       "24995      1  \n",
       "24996      1  \n",
       "24997      1  \n",
       "24998      1  \n",
       "24999      1  \n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['text'] = df_train['text'].apply(preprocessingString)\n",
    "df_test['text'] = df_test['text'].apply(preprocessingString)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandasToFeatures(df):\n",
    "    arr = df[\"text\"].apply(lambda x : getFeatures(x, lexicon)).values\n",
    "    index = np.arange(len(df_train['text'].values))\n",
    "    columns=['contains_no', 'count_pronouns', 'contains!', 'log_wordcount', 'nbr_positive', 'nbr_negative', 'negate_word']\n",
    "    \n",
    "    df_features = pd.DataFrame.from_dict(data=dict(zip(index, arr)), orient='index', columns=columns)\n",
    "    df_features['label'] = df['label']\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contains_no</th>\n",
       "      <th>count_pronouns</th>\n",
       "      <th>contains!</th>\n",
       "      <th>log_wordcount</th>\n",
       "      <th>nbr_positive</th>\n",
       "      <th>nbr_negative</th>\n",
       "      <th>negate_word</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>5.662960</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>5.407172</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>4.510860</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>4.795791</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>5.743003</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>4.682131</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>5.220356</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>4.919981</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>1</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>6.549651</td>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>4.025352</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       contains_no  count_pronouns  contains!  log_wordcount  nbr_positive  \\\n",
       "0                1             100          0       5.662960             7   \n",
       "1                1              87          0       5.407172             6   \n",
       "2                1              43          0       4.510860             3   \n",
       "3                1              50          0       4.795791             5   \n",
       "4                1             112          0       5.743003             4   \n",
       "...            ...             ...        ...            ...           ...   \n",
       "24995            1              43          0       4.682131             7   \n",
       "24996            1              72          0       5.220356             8   \n",
       "24997            1              39          0       4.919981            10   \n",
       "24998            1             240          0       6.549651            27   \n",
       "24999            0              17          0       4.025352             2   \n",
       "\n",
       "       nbr_negative  negate_word  label  \n",
       "0                 6            1      0  \n",
       "1                 4            4      0  \n",
       "2                 3            1      0  \n",
       "3                 5            1      0  \n",
       "4                11            1      0  \n",
       "...             ...          ...    ...  \n",
       "24995             3            0      1  \n",
       "24996             6            1      1  \n",
       "24997             3            0      1  \n",
       "24998            13            5      1  \n",
       "24999             1            0      1  \n",
       "\n",
       "[25000 rows x 8 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_features = pandasToFeatures(df_train)\n",
    "df_test_features = pandasToFeatures(df_train)\n",
    "df_train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18000, 7]) torch.Size([2000, 7]) torch.Size([5000, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  1.,  81.,   0.,  ...,   8.,  11.,   1.],\n",
       "        [  0.,  48.,   0.,  ...,   9.,   2.,   0.],\n",
       "        [  1.,  61.,   0.,  ...,  10.,   3.,   2.],\n",
       "        ...,\n",
       "        [  1.,  42.,   0.,  ...,  12.,   0.,   2.],\n",
       "        [  1., 125.,   0.,  ...,  15.,   6.,   5.],\n",
       "        [  1.,  49.,   0.,  ...,   3.,  10.,   0.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_col = df_train_features.columns[:-1]\n",
    "all_points = torch.tensor(df_train_features[train_col].values, dtype=torch.float32)\n",
    "labels = torch.tensor(df_train_features['label'].values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_points,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    stratify=labels,\n",
    "    random_state=42,\n",
    ")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.1,\n",
    "    stratify=y_train,\n",
    "    random_state=42,\n",
    ")\n",
    "print(X_train.shape, X_valid.shape, X_test.shape)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18000, 7]) torch.Size([18000, 1]) torch.Size([2000, 7]) torch.Size([2000, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    \"\"\"A logistic regression implementation\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, nb_classes: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: the dimension of the input features.\n",
    "            nb_classes: the number of classes to predict.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        output_layer = nn.Sigmoid() if nb_classes == 1 else nn.Softmax()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(input_dim, nb_classes),\n",
    "            output_layer,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: the input tensor.\n",
    "        Returns:\n",
    "            The output of activation function.\n",
    "        \"\"\"\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(7, 1)\n",
    "criterion = nn.BCELoss()  # Binary cross entropy\n",
    "# Stochastic gradient descent\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8974, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7925, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6950, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6416, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6128, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5969, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5879, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5827, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5795, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5776, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5764, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5757, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5753, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5750, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5749, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5749, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5749, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5750, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5751, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5751, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5752, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5753, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5754, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5755, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5755, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5756, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5757, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5757, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5757, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5758, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5758, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5759, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5759, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5759, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5759, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5760, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5760, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5760, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5760, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5760, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5760, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5760, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5760, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5760, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5760, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5760, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5760, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5760, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5760, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5760, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "CPU times: user 17.8 s, sys: 104 ms, total: 17.9 s\n",
      "Wall time: 2.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def train(model, criterion, optimizer, X_train, y_train, X_valid, y_valid, n_epochs=1000, pretty_print=True):\n",
    "    # Keeping an eye on the losses\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        # Setting all gradients to zero.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Sending the whole training set through the model.\n",
    "        predictions = model(X_train)\n",
    "        # Computing the loss.\n",
    "        loss = criterion(predictions, y_train)\n",
    "        train_losses.append(loss.item())\n",
    "        if epoch % 100 == 0 and pretty_print:\n",
    "            print(loss)\n",
    "        # Computing the gradients and gradient descent.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # When computing the validation loss, we do not want to update the weights.\n",
    "        # torch.no_grad tells PyTorch to not save the necessary data used for\n",
    "        # gradient descent.\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_valid)\n",
    "            loss = criterion(predictions, y_valid)\n",
    "            test_losses.append(loss)\n",
    "    return train_losses, test_losses\n",
    "\n",
    "train_losses, test_losses = train(model, criterion, optimizer, X_train, y_train, X_valid, y_valid, n_epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training loss:  2.897380828857422\n",
      "Final training loss:  0.5760493874549866\n",
      "First test loss:  tensor(1.4642)\n",
      "Final test loss:  tensor(0.5801)\n",
      "5000 5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd9a0a80310>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"First training loss: \", train_losses[0])\n",
    "print(\"Final training loss: \", train_losses[-1])\n",
    "print(\"First test loss: \", test_losses[0])\n",
    "print(\"Final test loss: \", test_losses[-1])\n",
    "print(len(train_losses), len(test_losses))\n",
    "\n",
    "\n",
    "# Checking the losses\n",
    "plt.plot(np.arange(len(train_losses)), train_losses, label=\"Training loss\")\n",
    "plt.plot(np.arange(len(test_losses)), test_losses, label=\"Test loss\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate your classifier in terms of accuracy for the training, validation, and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7266666666666667 0.703 0.713\n"
     ]
    }
   ],
   "source": [
    "# Computing the accuracy for our 3 splits.\n",
    "with torch.no_grad():\n",
    "    p_train = model(X_train)\n",
    "    p_train = np.round(p_train.numpy())\n",
    "    training_accuracy = np.mean(p_train == y_train.numpy())\n",
    "    p_valid = model(X_valid)\n",
    "    p_valid = np.round(p_valid.numpy())\n",
    "    valid_accuracy = np.mean(p_valid == y_valid.numpy())\n",
    "    p_test = model(X_test)\n",
    "    p_test = np.round(p_test.numpy())\n",
    "    test_accuracy = np.mean(p_test == y_test.numpy())\n",
    "print(training_accuracy, valid_accuracy, test_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the classifier is more accurate than random on all the datasets although it is less accurate than our Naive Bayes Classifier. \n",
    "The validation set accuracy and the testing set accuracy are quite close so our model does not overfit a lot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the weights of your classifier. Which features seems to play most for both classes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0023,  0.0074, -0.0159, -0.0457,  0.1019, -0.1755, -0.0956]]),\n",
       " tensor([-0.0039]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the model's weights and bias.\n",
    "model.classifier[0].state_dict()[\"weight\"], model.classifier[0].state_dict()[\"bias\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the key feature used to detect if a sentence is positive is the number of words in the document wich are in the positive lexicon\n",
    "\n",
    "not so surprisingly the key feature to detect if a sentence is negative is the number of words in the document wich are in the negative lexicon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Bonus] The parameter weight_decay of the SGD optimizer corresponds to the L2 penalty. Try playing with this value and explain how it influence the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight decay:  0.01\n",
      "tensor([[-0.0357,  0.0103, -0.2676, -0.0665,  0.1410, -0.1840, -0.2996]]) tensor([-0.1076])\n",
      "Weight decay:  0.1\n",
      "tensor([[ 0.2491,  0.0056,  0.2014, -0.0631,  0.1076, -0.2245,  0.1093]]) tensor([-0.0898])\n",
      "Weight decay:  0.5\n",
      "tensor([[-0.0550,  0.0042,  0.0183, -0.0056,  0.0937, -0.1724,  0.0010]]) tensor([0.0156])\n",
      "Weight decay:  1\n",
      "tensor([[-0.0340,  0.0050,  0.0984, -0.0174,  0.0926, -0.1496, -0.0538]]) tensor([-0.0903])\n",
      "Weight decay:  2\n",
      "tensor([[-0.0484,  0.0070,  0.0243, -0.0116,  0.0754, -0.1303, -0.0230]]) tensor([0.0183])\n",
      "Weight decay:  5\n",
      "tensor([[-0.0038, -0.0056,  0.0015, -0.0123,  0.0590, -0.0955, -0.0193]]) tensor([-0.0006])\n",
      "Weight decay:  10\n",
      "tensor([[-2.3294e-03, -8.3158e-03, -4.2849e-06, -6.8166e-03,  4.1279e-02,\n",
      "         -6.4517e-02, -1.0735e-02]]) tensor([-0.0013])\n",
      "Weight decay:  20\n",
      "tensor([[-1.0573e-03,  1.1570e-02, -3.6841e-10, -2.6061e-03,  2.7620e-02,\n",
      "         -3.6591e-02, -5.2267e-03]]) tensor([-0.0005])\n",
      "Weight decay:  100\n",
      "tensor([[-3.3155e-04, -1.1361e-02,  5.6052e-45, -1.1942e-03,  4.7183e-03,\n",
      "         -9.7403e-03, -1.4199e-03]]) tensor([-0.0002])\n"
     ]
    }
   ],
   "source": [
    "# param grid for weight decay\n",
    "param_grid = {\"weight_decay\": [0.01, 0.1, 0.5, 1, 2, 5, 10, 20, 100]}\n",
    "# Grid search\n",
    "for elt in param_grid[\"weight_decay\"]:\n",
    "    model_w = LogisticRegression(7, 1)\n",
    "    criterion_w = nn.BCELoss()  # Binary cross entropy\n",
    "    optimizer_w = torch.optim.SGD(model_w.parameters(), lr=0.001, weight_decay=elt)\n",
    "    train_losses, test_losses = train(model_w, criterion_w, optimizer_w, X_train, y_train, X_valid, y_valid, n_epochs=1000, pretty_print=False)\n",
    "    print(\"Weight decay: \", elt)\n",
    "    print(model_w.classifier[0].state_dict()[\"weight\"], model_w.classifier[0].state_dict()[\"bias\"])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "increasing the weight decay parameter let the weights have much more extreme values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take two wrongly classified samples in the test set and try explaining why the model was wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature meaning : [contains_no, count_pronoun, contains!, log_wordcount, nbr_positive, nbr_negative, negate_word\t]\n",
      "feature for example 1 :  [1, 40, 0, 4.912654885736052, 12, 2, 4]\n",
      "feature for example 2 :  [1, 47, 0, 4.882801922586371, 7, 9, 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>ai_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>first off let me say  if you haven t enjoyed a van damme movie since bloodsport  you probably will not like this movie  most of these movies may not have the best plots or best actors but i enjoy these kinds of movies for what they are  this movie is much better than any of the movies the other action guys  segal and dolph  have thought about putting out the past few years  van damme is good in the movie  the movie is only worth watching to van damme fans  it is not as good as wake of death  which i highly recommend to anyone of likes van damme  or in hell but  in my opinion it s worth watching  it has the same type of feel to it as nowhere to run  good fun stuff</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>i caught this movie on the sci-fi channel recently  it actually turned out to be pretty decent as far as b-list horror suspense films go  two guys  one naive and one loud mouthed a    take a road trip to stop a wedding but have the worst possible luck when a maniac in a freaky  make-shift tank truck hybrid decides to play cat-and-mouse with them  things are further complicated when they pick up a ridiculously whorish hitchhiker  what makes this film unique is that the combination of comedy and terror actually work in this movie  unlike so many others  the two guys are likable enough and there are some good chase suspense scenes  nice pacing and comic timing make this movie more than passable for the horror slasher buff  definitely worth checking out</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           text  \\\n",
       "4                                                                                              first off let me say  if you haven t enjoyed a van damme movie since bloodsport  you probably will not like this movie  most of these movies may not have the best plots or best actors but i enjoy these kinds of movies for what they are  this movie is much better than any of the movies the other action guys  segal and dolph  have thought about putting out the past few years  van damme is good in the movie  the movie is only worth watching to van damme fans  it is not as good as wake of death  which i highly recommend to anyone of likes van damme  or in hell but  in my opinion it s worth watching  it has the same type of feel to it as nowhere to run  good fun stuff    \n",
       "24999  i caught this movie on the sci-fi channel recently  it actually turned out to be pretty decent as far as b-list horror suspense films go  two guys  one naive and one loud mouthed a    take a road trip to stop a wedding but have the worst possible luck when a maniac in a freaky  make-shift tank truck hybrid decides to play cat-and-mouse with them  things are further complicated when they pick up a ridiculously whorish hitchhiker  what makes this film unique is that the combination of comedy and terror actually work in this movie  unlike so many others  the two guys are likable enough and there are some good chase suspense scenes  nice pacing and comic timing make this movie more than passable for the horror slasher buff  definitely worth checking out    \n",
       "\n",
       "       label  ai_pred  \n",
       "4          0        1  \n",
       "24999      1        0  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take wrong predictions and look at them\n",
    "with torch.no_grad():\n",
    "    y_pred = df_test['text'].apply(lambda text : (int)(np.round(model(torch.tensor(getFeatures(text, lexicon), dtype=torch.float32)))))\n",
    "df_wrong_pred = df_test[y_pred != df_test['label']].loc[:, ['text', 'label']]\n",
    "df_wrong_pred['ai_pred'] = y_pred.loc[y_pred!= df_test['label']]\n",
    "\n",
    "# take wrong predictions and look at their features\n",
    "print(\"feature meaning : [contains_no, count_pronoun, contains!, log_wordcount, nbr_positive, nbr_negative, negate_word\t]\")\n",
    "print(\"feature for example 1 : \", getFeatures(df_wrong_pred.iloc[0]['text'], lexicon))\n",
    "print(\"feature for example 2 : \", getFeatures(df_wrong_pred.iloc[-1]['text'], lexicon))\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_wrong_pred.iloc[[0, -1]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is two examples where the program should have return a class 0 and 1 (negative, positive).  \n",
    "\n",
    "the first one is a negative review but our model predicted it was a positive review because there is a lot of positive words in this example like \"best\" or \"like\" \"better\" \"good\" although they are used in a negative way or to compare to a better movie \n",
    "\n",
    "the second one is a positive review but our model predicted it was negative, it is most likely because they are more negative words than positives one in this review, these negative words are used to describe the story not top criticize the movie but the model does not know that\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit :  0.715 pytorch :  0.713\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn implementation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Training the model\n",
    "clf = LogisticRegression(random_state=42, max_iter=5000)\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "\n",
    "# Checking the accuracy\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# compare pytorch and scikit-learn accuracy\n",
    "print(\"scikit : \" ,accuracy_score(y_test, y_pred),\"pytorch : \", test_accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both implementations seems to have the same results with the same datasets and feature vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea9312fc9c2d6329322094b403542e3a8436f2615ce450c7872cc3a27bdb75bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
