{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e402d1-f6f5-4ee5-8cde-813c2a2b4ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from typing import Callable, Dict, Generator, List, Tuple\n",
    "\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da5f4d3c",
   "metadata": {},
   "source": [
    "# FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f931ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a13d7994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/pierre/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004024ba0ec94676888dec3d4afc01bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/pierre/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-5f37fd0866e4f89f.arrow and /home/pierre/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-dd5732a0e6ac784c.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((20000, 2), (5000, 2), (25000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "train_dataset = dataset[\"train\"].train_test_split(\n",
    "    stratify_by_column=\"label\", test_size=0.2, seed=42\n",
    ")\n",
    "test_df = dataset[\"test\"]\n",
    "train_df = train_dataset[\"train\"]\n",
    "valid_df = train_dataset[\"test\"]\n",
    "train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28d438f7",
   "metadata": {},
   "source": [
    "1. (2 points) Turn the dataset into a dataset compatible with Fastext (see the Tips on using FastText section a bit lower).\n",
    "For pretreatment, only apply lower casing and punctuation removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "149d25dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__positive after reading the other tepid reviews and comments  i felt i had to come to bat for this movie   roeg s films tend to have little to do with one another  and expecting this one to be like one of his you liked is probably off the mark   what this film is is a thoughtful and unabashed look at religious faith  the only other film like it-in terms of its religious message-would have to be tolkin s  the rapture    i am astonished that anyone could say the story is muddled or supernatural  it is a simple movie about catholic faith  miracles  and redemption--though you would never guess it till the end  it is also the only movie i can think of whose resolution turns  literally  on a pun   as a  happily  fallen catholic myself  i know what the movie is about  and i find a sort of fondness in its ultimate innocence about the relation between god and man  but if you are not familiar with the kind of theology on which the film is based  then it will go right over you head   as a film-as opposed to a story- cold heaven  it is not ground-breaking  while  the rapture  is heavy with pictorial significance and cinematic imagery   cold heaven  downplays its own cinematic qualities  there are no striking shots  no edgy effects  no attempts to fit the content to the form  it is workmanlike shooting  but subdued  nor does it have dialogue or acting to put it in a class of high drama  it is a simple story that unfolds simply  it may seem odd  but at the end the mystery is revealed  it looks ambiguous  but with a single line the ambiguity vanishes in a puff of catholic dogma   in this regard   cold heaven  has at its heart exactly the same sort of thing that drives a movie like  the sting   or  the sixth sense   or  final descent   or polanski s  a pure formality   all of these are films with a trick up their sleeves  they may frustrate you along the way  but they have a point-an obvious one  indeed--but the fun is  at least in part  in having been taken in   still  even if it seems like little more than a shaggy dog story with a punch line  it is worth watching for way it directs-and misdirects-you  try it-especially if you are  or have ever been  a catholic \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# turn the data into fasttext format : __label__0/1 + text => __label__positive/negative + text\n",
    "def format_label(label: int) -> str:\n",
    "    \"\"\"\n",
    "        Return the label in the fasttext format.\n",
    "    \"\"\"\n",
    "    if label == 0:\n",
    "        return \"__label__negative\"\n",
    "    return \"__label__positive\"\n",
    "\n",
    "def preprocessingString(text: str) -> str:\n",
    "    '''\n",
    "        Preprocess a string. Remove punctuation and lower the text.\n",
    "    '''\n",
    "    text = text.lower().replace(\"<br />\", \" \")\n",
    "    for punct in string.punctuation:\n",
    "        if (not punct in str(\"-\")):\n",
    "            text = text.replace(punct, \" \")\n",
    "    return text\n",
    "\n",
    "def fasttext_format(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "        Return a list of strings in the fasttext format.\n",
    "    \"\"\"\n",
    "    return [format_label(label) + \" \" + preprocessingString(text) + \"\\n\" for text, label in zip(df['text'], df['label'])]\n",
    "\n",
    "def write_fasttext_file(df: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"\n",
    "        Write a file in the fasttext format.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        l = fasttext_format(df)\n",
    "        random.shuffle(l)\n",
    "        # sort the labels to have positive first, then negative\n",
    "        l.sort(key=lambda x: x.split()[0], reverse=True)\n",
    "        f.writelines(l)\n",
    "\n",
    "# write the files\n",
    "write_fasttext_file(train_df, \"train.txt\")\n",
    "write_fasttext_file(valid_df, \"valid.txt\")\n",
    "write_fasttext_file(test_df, \"test.txt\")\n",
    "\n",
    "\n",
    "# print the first line of the file to check\n",
    "with open(\"train.txt\", \"r\") as f:\n",
    "    print(f.readlines()[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e2a90ef",
   "metadata": {},
   "source": [
    "2. (2 points) Train a FastText classifier with default parameters on the training data, and evaluate it on the test data using accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba0874c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 4M words\n",
      "Number of words:  86069\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 2654061 lr:  0.000000 avg.loss:  0.359210 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set: 0.9028\n",
      "Accuracy on the test set: 0.8756\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model = fasttext.train_supervised(input=\"train.txt\")\n",
    "\n",
    "# print the accuracy\n",
    "print(f\"Accuracy on the training set: {model.test('train.txt')[1]}\")\n",
    "print(f\"Accuracy on the test set: {model.test('test.txt')[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b3f93d4",
   "metadata": {},
   "source": [
    "3. (2 points) Use the [hyperparameters search functionality](https://fasttext.cc/docs/en/autotune.html) of FastText and repeat step 2.\n",
    "   * To do so, you'll need to [split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) your training set into a training and a validation set.\n",
    "   * Let the model search for 5 minutes (it's the default search time).\n",
    "   * Don't forget to shuffle (and stratify) your splits. The dataset has its entry ordered by label (0s first, then 1s). Feeding the classifier one class and then the second can mess with its performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0ca00d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:    9 Best score:  0.895800 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 4M words\n",
      "Number of words:  86069\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1215143 lr:  0.000000 avg.loss:  0.042706 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set: 1.0\n",
      "Accuracy on the validation set: 0.8972\n",
      "Accuracy on the test set: 0.8936\n"
     ]
    }
   ],
   "source": [
    "# train using hyperparameters search\n",
    "hyper_model = fasttext.train_supervised(input=\"train.txt\", autotuneValidationFile=\"valid.txt\")\n",
    "\n",
    "# print the accuracy\n",
    "print(f\"Accuracy on the training set: {hyper_model.test('train.txt')[1]}\")\n",
    "print(f\"Accuracy on the validation set: {hyper_model.test('valid.txt')[1]}\")\n",
    "print(f\"Accuracy on the test set: {hyper_model.test('test.txt')[1]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59f834e5",
   "metadata": {},
   "source": [
    "4. (1 points) Look at the differences between the default model and the attributes found with hyperparameters search. How do the two models differ?\n",
    "   * Only refer to the attributes you think are interesting.\n",
    "   * See the _Tips on using FastText_ (just below) for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53bda358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model attributes vs Second model attributes:\n",
      "model lr:            0.1                            | hyper_model lr:              0.08499425639667486\n",
      "model dim:           100                            | hyper_model dim:             92\n",
      "model epoch:         5                              | hyper_model epoch:           100\n",
      "model minCount:      1                              | hyper_model minCount:        1\n",
      "model minCountLabel: 0                              | hyper_model minCountLabel:   0\n",
      "model minn:          0                              | hyper_model minn:            0\n",
      "model maxn:          0                              | hyper_model maxn:            0\n",
      "model neg:           5                              | hyper_model neg:             5\n",
      "model wordNgrams:    1                              | hyper_model wordNgrams:      2\n",
      "model loss:          loss_name.softmax              | hyper_model loss:            loss_name.softmax\n"
     ]
    }
   ],
   "source": [
    "# check model attributes\n",
    "# pretty print with padding\n",
    "print(\"Model attributes vs Second model attributes:\")\n",
    "print(\"model lr:\".ljust(20), str(model.lr).ljust(30), \"| hyper_model lr:\".ljust(30), hyper_model.lr)\n",
    "print(\"model dim:\".ljust(20), str(model.dim).ljust(30), \"| hyper_model dim:\".ljust(30), hyper_model.dim)\n",
    "print(\"model epoch:\".ljust(20), str(model.epoch).ljust(30), \"| hyper_model epoch:\".ljust(30), hyper_model.epoch)\n",
    "print(\"model minCount:\".ljust(20), str(model.minCount).ljust(30), \"| hyper_model minCount:\".ljust(30), hyper_model.minCount)\n",
    "print(\"model minCountLabel:\".ljust(20), str(model.minCountLabel).ljust(30), \"| hyper_model minCountLabel:\".ljust(30), hyper_model.minCountLabel)\n",
    "print(\"model minn:\".ljust(20), str(model.minn).ljust(30), \"| hyper_model minn:\".ljust(30), hyper_model.minn)\n",
    "print(\"model maxn:\".ljust(20), str(model.maxn).ljust(30), \"| hyper_model maxn:\".ljust(30), hyper_model.maxn)\n",
    "print(\"model neg:\".ljust(20), str(model.neg).ljust(30), \"| hyper_model neg:\".ljust(30), hyper_model.neg)\n",
    "print(\"model wordNgrams:\".ljust(20), str(model.wordNgrams).ljust(30), \"| hyper_model wordNgrams:\".ljust(30), hyper_model.wordNgrams)\n",
    "print(\"model loss:\".ljust(20), str(model.loss).ljust(30), \"| hyper_model loss:\".ljust(30), hyper_model.loss)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ae2bef5",
   "metadata": {},
   "source": [
    "The learning rate (0.1 vs 0.085)  and the dim (100 vs 92) differs a bit, maybe these affined values are best suited for the problem than the default ones although they are still close  \n",
    "the second model trains on a lot more epoch (5 vs 100), wich increases the accuracy on the train set but it leads to a bit of overtraining"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbad84c9",
   "metadata": {},
   "source": [
    "5. (1 point) Using the tuned model, take at least 2 wrongly classified examples from the test set, and try explaining why the model failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09e9a58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line :  __label__negative to truly appreciate this film you had to be there  acting   or have been a crew member   yes  i am  selena   and at the ripe old age of 42  have serious doubts about what we were doing did   it all started out to be like a  john waters  type thing  friends acting badly in bad films  somewhere along the line the fun discontinued  people who were supposed to be friends didn t speak anymore  and bad became worse   i regret the bad image i might have projected  try to fit in size one gold spandex pants    other than that  the film sucks so badly  i would not even make my mama watch it   to my director  cast and crew i say    why can t we just all get along    it s been over twenty years  people                    \n",
      "\n",
      "Prediction :  __label__positive \n",
      "Label :  __label__negative \n",
      "\n",
      "Line :  __label__negative i couldn t even sit through the whole thing  this movie was a piece of crap  i had more fun watching  dont  tell mom the babysitter s dead   it was just too painful to watch  say  besides  austin powers   has tom arnold ever been in a hit movie \n",
      "\n",
      "Prediction :  __label__positive \n",
      "Label :  __label__negative \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take two wrong predictions\n",
    "wrong_predictions = []\n",
    "labels = []\n",
    "with open(\"test.txt\", \"r\") as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        predicted_label = format_label(hyper_model.predict(line.split()[1:])[0][0])\n",
    "        true_label = line.split()[0]\n",
    "        if predicted_label != true_label:\n",
    "            print(\"Line : \", line)\n",
    "            print(\"Prediction : \", predicted_label, \"\\nLabel : \", true_label, \"\\n\")\n",
    "\n",
    "            wrong_predictions.append(line)\n",
    "            labels.append(line.split()[0])\n",
    "            if len(wrong_predictions) == 2:\n",
    "                break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a038dee",
   "metadata": {},
   "source": [
    "# TODO : expliquer pk c'est des mauvaises predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a31f1b47",
   "metadata": {},
   "source": [
    "6. (Bonus point) Why is it likely that the attributes `minn` and `maxn` are at 0 after an hyperparameter search on our data?\n",
    "   * Hint: on what language are we working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20c852d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model minn:          0                              | hyper_model minn:            0\n",
      "model maxn:          0                              | hyper_model maxn:            0\n"
     ]
    }
   ],
   "source": [
    "print(\"model minn:\".ljust(20), str(model.minn).ljust(30), \"| hyper_model minn:\".ljust(30), hyper_model.minn)\n",
    "print(\"model maxn:\".ljust(20), str(model.maxn).ljust(30), \"| hyper_model maxn:\".ljust(30), hyper_model.maxn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea9312fc9c2d6329322094b403542e3a8436f2615ce450c7872cc3a27bdb75bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
