{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e402d1-f6f5-4ee5-8cde-813c2a2b4ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pili/anaconda3/envs/scia/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from typing import Callable, Dict, Generator, List, Tuple\n",
    "\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da5f4d3c",
   "metadata": {},
   "source": [
    "# FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f931ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7da5404a",
   "metadata": {},
   "source": [
    "# Datasets and splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a13d7994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/pili/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|██████████| 3/3 [00:00<00:00, 850.02it/s]\n",
      "Loading cached split indices for dataset at /home/pili/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-5f37fd0866e4f89f.arrow and /home/pili/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-dd5732a0e6ac784c.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((20000, 2), (5000, 2), (25000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "train_dataset = dataset[\"train\"].train_test_split(\n",
    "    stratify_by_column=\"label\", test_size=0.2, seed=42\n",
    ")\n",
    "test_df = dataset[\"test\"]\n",
    "train_df = train_dataset[\"train\"]\n",
    "valid_df = train_dataset[\"test\"]\n",
    "train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28d438f7",
   "metadata": {},
   "source": [
    "1. (2 points) Turn the dataset into a dataset compatible with Fastext (see the Tips on using FastText section a bit lower).\n",
    "For pretreatment, only apply lower casing and punctuation removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "149d25dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__positive sterling and younger brother try to survive on land  being squeezed by big cattlemen  when  rogue  brother preston arrives  a moral dilemma ensues  john  drew  barrymore steals the show as the younger  impressionable brother-barrymore shows signs here that he could have been an acting powerhouse  moves at a nice pace to an exciting climax \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# turn the data into fasttext format : __label__0/1 + text => __label__positive/negative + text\n",
    "def format_label(label: int) -> str:\n",
    "    \"\"\"\n",
    "        Return the label in the fasttext format.\n",
    "    \"\"\"\n",
    "    if label == 0:\n",
    "        return \"__label__negative\"\n",
    "    return \"__label__positive\"\n",
    "\n",
    "def preprocessingString(text: str) -> str:\n",
    "    '''\n",
    "        Preprocess a string. Remove punctuation and lower the text.\n",
    "    '''\n",
    "    text = text.lower().replace(\"<br />\", \" \")\n",
    "    for punct in string.punctuation:\n",
    "        if (not punct in str(\"-\")):\n",
    "            text = text.replace(punct, \" \")\n",
    "    return text\n",
    "\n",
    "def fasttext_format(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "        Return a list of strings in the fasttext format.\n",
    "    \"\"\"\n",
    "    return [format_label(label) + \" \" + preprocessingString(text) + \"\\n\" for text, label in zip(df['text'], df['label'])]\n",
    "\n",
    "def write_fasttext_file(df: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"\n",
    "        Write a file in the fasttext format.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        l = fasttext_format(df)\n",
    "        random.shuffle(l)\n",
    "        # sort the labels to have positive first, then negative\n",
    "        l.sort(key=lambda x: x.split()[0], reverse=True)\n",
    "        f.writelines(l)\n",
    "\n",
    "# write the files\n",
    "write_fasttext_file(train_df, \"train.txt\")\n",
    "write_fasttext_file(valid_df, \"valid.txt\")\n",
    "write_fasttext_file(test_df, \"test.txt\")\n",
    "\n",
    "\n",
    "# print the first line of the file to check\n",
    "with open(\"train.txt\", \"r\") as f:\n",
    "    print(f.readlines()[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e2a90ef",
   "metadata": {},
   "source": [
    "2. (2 points) Train a FastText classifier with default parameters on the training data, and evaluate it on the test data using accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba0874c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 4M words\n",
      "Number of words:  86069\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 3100510 lr:  0.000000 avg.loss:  0.401143 ETA:   0h 0m 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set: 0.8959\n",
      "Accuracy on the test set: 0.8696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model = fasttext.train_supervised(input=\"train.txt\")\n",
    "\n",
    "# print the accuracy\n",
    "print(f\"Accuracy on the training set: {model.test('train.txt')[1]}\")\n",
    "print(f\"Accuracy on the test set: {model.test('test.txt')[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b3f93d4",
   "metadata": {},
   "source": [
    "3. (2 points) Use the [hyperparameters search functionality](https://fasttext.cc/docs/en/autotune.html) of FastText and repeat step 2.\n",
    "   * To do so, you'll need to [split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) your training set into a training and a validation set.\n",
    "   * Let the model search for 5 minutes (it's the default search time).\n",
    "   * Don't forget to shuffle (and stratify) your splits. The dataset has its entry ordered by label (0s first, then 1s). Feeding the classifier one class and then the second can mess with its performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0ca00d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:    9 Best score:  0.895800 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 4M words\n",
      "Number of words:  86069\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1065783 lr:  0.000000 avg.loss:  0.036241 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set: 1.0\n",
      "Accuracy on the validation set: 0.896\n",
      "Accuracy on the test set: 0.89584\n"
     ]
    }
   ],
   "source": [
    "# train using hyperparameters search\n",
    "hyper_model = fasttext.train_supervised(input=\"train.txt\", autotuneValidationFile=\"valid.txt\")\n",
    "\n",
    "# print the accuracy\n",
    "print(f\"Accuracy on the training set: {hyper_model.test('train.txt')[1]}\")\n",
    "print(f\"Accuracy on the validation set: {hyper_model.test('valid.txt')[1]}\")\n",
    "print(f\"Accuracy on the test set: {hyper_model.test('test.txt')[1]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59f834e5",
   "metadata": {},
   "source": [
    "4. (1 points) Look at the differences between the default model and the attributes found with hyperparameters search. How do the two models differ?\n",
    "   * Only refer to the attributes you think are interesting.\n",
    "   * See the _Tips on using FastText_ (just below) for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53bda358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model attributes vs Second model attributes:\n",
      "model lr:            0.1                            | hyper_model lr:              0.08499425639667486\n",
      "model dim:           100                            | hyper_model dim:             92\n",
      "model epoch:         5                              | hyper_model epoch:           100\n",
      "model minCount:      1                              | hyper_model minCount:        1\n",
      "model minCountLabel: 0                              | hyper_model minCountLabel:   0\n",
      "model minn:          0                              | hyper_model minn:            0\n",
      "model maxn:          0                              | hyper_model maxn:            0\n",
      "model neg:           5                              | hyper_model neg:             5\n",
      "model wordNgrams:    1                              | hyper_model wordNgrams:      2\n",
      "model loss:          loss_name.softmax              | hyper_model loss:            loss_name.softmax\n"
     ]
    }
   ],
   "source": [
    "# check model attributes\n",
    "# pretty print with padding\n",
    "print(\"Model attributes vs Second model attributes:\")\n",
    "print(\"model lr:\".ljust(20), str(model.lr).ljust(30), \"| hyper_model lr:\".ljust(30), hyper_model.lr)\n",
    "print(\"model dim:\".ljust(20), str(model.dim).ljust(30), \"| hyper_model dim:\".ljust(30), hyper_model.dim)\n",
    "print(\"model epoch:\".ljust(20), str(model.epoch).ljust(30), \"| hyper_model epoch:\".ljust(30), hyper_model.epoch)\n",
    "print(\"model minCount:\".ljust(20), str(model.minCount).ljust(30), \"| hyper_model minCount:\".ljust(30), hyper_model.minCount)\n",
    "print(\"model minCountLabel:\".ljust(20), str(model.minCountLabel).ljust(30), \"| hyper_model minCountLabel:\".ljust(30), hyper_model.minCountLabel)\n",
    "print(\"model minn:\".ljust(20), str(model.minn).ljust(30), \"| hyper_model minn:\".ljust(30), hyper_model.minn)\n",
    "print(\"model maxn:\".ljust(20), str(model.maxn).ljust(30), \"| hyper_model maxn:\".ljust(30), hyper_model.maxn)\n",
    "print(\"model neg:\".ljust(20), str(model.neg).ljust(30), \"| hyper_model neg:\".ljust(30), hyper_model.neg)\n",
    "print(\"model wordNgrams:\".ljust(20), str(model.wordNgrams).ljust(30), \"| hyper_model wordNgrams:\".ljust(30), hyper_model.wordNgrams)\n",
    "print(\"model loss:\".ljust(20), str(model.loss).ljust(30), \"| hyper_model loss:\".ljust(30), hyper_model.loss)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ae2bef5",
   "metadata": {},
   "source": [
    "The learning rate (0.1 vs 0.085)  and the dim (100 vs 92) differs a bit, maybe these affined values are best suited for the problem than the default ones although they are still close  \n",
    "the second model trains on a lot more epoch (5 vs 100), wich increases the accuracy on the train set but it leads to a bit of overtraining"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbad84c9",
   "metadata": {},
   "source": [
    "5. (1 point) Using the tuned model, take at least 2 wrongly classified examples from the test set, and try explaining why the model failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09e9a58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line :  __label__positive an interesting companion piece to true documentaries of john c  holmes  unfortunately  it doesn t deal with what ultimately killed holmes  and it certainly could have benefited from doing so  burt reynolds and mark wahlberg got the most praise for this  but i felt the true stars were julianne moore as the cocaine-sniffing mother wannabe  don cheadle as a black man struggling with identity as pornstar stereo-salesman in some wild getups and william h  macy  who s wife is the ultimate slut  not to mention a nearly unrecognizable alfred molina  macy s new year s eve bash and cheadle s chance for a better life after a donut shop robbery gone wildly wrong are probably the two best scenes in the movie  or at least the two best shot  what this movie does best is show how power can easily corrupt in its various forms  however  none of the characters apparently learn anything from their dark downward spiral as they all rebound and return to their normal lives \n",
      "\n",
      "Prediction :  __label__negative \n",
      "Label :  __label__positive \n",
      "\n",
      "Line :  __label__positive this is another film where the cinematography is the best thing to recommend it  that would be fine if the film were a travelogue  but as a dramatic exercise in cinematic artistry  that is not good enough  the theme of inter-species respect and co-operation ventures timidly into the forbidden world of inter-species love  but its approach is stereotypical  indicating a lack of understanding of the behavior motives of either species  as with many films  one always wonders what could have been achieved by a more innovative director and a more creative screenwriter  alas  we probably will never know \n",
      "\n",
      "Prediction :  __label__negative \n",
      "Label :  __label__positive \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take two wrong predictions\n",
    "wrong_predictions = []\n",
    "labels = []\n",
    "with open(\"test.txt\", \"r\") as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        predicted_label = hyper_model.predict(line.split()[1:])[0][0][0]\n",
    "        true_label = line.split()[0]\n",
    "        if predicted_label != true_label:\n",
    "            print(\"Line : \", line)\n",
    "            print(\"Prediction : \", predicted_label, \"\\nLabel : \", true_label, \"\\n\")\n",
    "\n",
    "            wrong_predictions.append(line)\n",
    "            labels.append(line.split()[0])\n",
    "            if len(wrong_predictions) == 2:\n",
    "                break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a038dee",
   "metadata": {},
   "source": [
    "#### Because of the shuffle the results are not reprodictable\n",
    "\n",
    "### Above we have two examples of wrong predictions :  \n",
    "#### The first one :  \n",
    "an interesting companion piece to true documentaries of john c  holmes  unfortunately  it doesn t deal with what ultimately killed holmes  and it certainly could have benefited from doing so  burt reynolds and mark wahlberg got the most praise for this  but i felt the true stars were julianne moore as the cocaine-sniffing mother wannabe  don cheadle as a black man struggling with identity as pornstar stereo-salesman in some wild getups and william h  macy  who s wife is the ultimate slut  not to mention a nearly unrecognizable alfred molina  macy s new year s eve bash and cheadle s chance for a better life after a donut shop robbery gone wildly wrong are probably the two best scenes in the movie  or at least the two best shot  what this movie does best is show how power can easily corrupt in its various forms  however  none of the characters apparently learn anything from their dark downward spiral as they all rebound and return to their normal lives\n",
    "\n",
    "#### is labelled as negative but our code labelled it as positive, surely because the review is neutral\n",
    "\n",
    "#### The second one :  \n",
    "this is another film where the cinematography is the best thing to recommend it  that would be fine if the film were a travelogue  but as a dramatic exercise in cinematic artistry  that is not good enough  the theme of inter-species respect and co-operation ventures timidly into the forbidden world of inter-species love  but its approach is stereotypical  indicating a lack of understanding of the behavior motives of either species  as with many films  one always wonders what could have been achieved by a more innovative director and a more creative screenwriter  alas  we probably will never know \n",
    "\n",
    "#### is labelled as negative but our code labelled it as positive, it may be because the review starts with a simingly very positive sentence\n",
    "￼Votre réponse\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a31f1b47",
   "metadata": {},
   "source": [
    "6. (Bonus point) Why is it likely that the attributes `minn` and `maxn` are at 0 after an hyperparameter search on our data?\n",
    "   * Hint: on what language are we working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20c852d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model minn:          0                              | hyper_model minn:            0\n",
      "model maxn:          0                              | hyper_model maxn:            0\n"
     ]
    }
   ],
   "source": [
    "print(\"model minn:\".ljust(20), str(model.minn).ljust(30), \"| hyper_model minn:\".ljust(30), hyper_model.minn)\n",
    "print(\"model maxn:\".ljust(20), str(model.maxn).ljust(30), \"| hyper_model maxn:\".ljust(30), hyper_model.maxn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "916f4f6d",
   "metadata": {},
   "source": [
    "The little we understood about maxn and minn is that they refers to the char n-grams, a char n-gram is a n-length sequence from a word, since we are working in english woth no defined length for words it may seems useless in our case and it may expalin why they stay at 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea9312fc9c2d6329322094b403542e3a8436f2615ce450c7872cc3a27bdb75bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
