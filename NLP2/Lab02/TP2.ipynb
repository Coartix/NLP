{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List\n",
        "from torch import Tensor\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "import portalocker\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "print(DEVICE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab02 Encoder-decoder model\n",
        "\n",
        "To start off we will do the rewriting of the tutorial of pyTorch language translation with nn.Transformer and torchtext."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Language Translation with ``nn.Transformer`` and torchtext\n",
        "\n",
        "This tutorial shows:\n",
        "    - How to train a translation model from scratch using Transformer.\n",
        "    - Use torchtext library to access  [Multi30k](http://www.statmt.org/wmt16/multimodal-task.html#task1)_ dataset to train a German to English translation model.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Sourcing and Processing\n",
        "\n",
        "[torchtext library](https://pytorch.org/text/stable/)_ has utilities for creating datasets that can be easily\n",
        "iterated through for the purposes of creating a language translation\n",
        "model. In this example, we show how to use torchtext's inbuilt datasets,\n",
        "tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor. We will use\n",
        "[Multi30k dataset from torchtext library](https://pytorch.org/text/stable/datasets.html#multi30k)_\n",
        "that yields a pair of source-target raw sentences.\n",
        "\n",
        "To access torchtext datasets, please install torchdata following instructions at https://github.com/pytorch/data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
        "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create source and target language tokenizer. Make sure to install the dependencies.\n",
        "\n",
        "```python\n",
        "pip install spacy sacrebleu torchdata -U\n",
        "python -m spacy download en_core_web_sm\n",
        "python -m spacy download de_core_news_sm\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/coartix/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-05-29 17:44:24.644628: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-29 17:44:25.307346: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-29 17:44:29.645218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
          ]
        }
      ],
      "source": [
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Training data Iterator\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    # Create torchtext's Vocab object\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "\n",
        "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
        "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Seq2Seq Network using Transformer\n",
        "\n",
        "Transformer is a Seq2Seq model introduced in [“Attention is all you\n",
        "need”](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)_\n",
        "paper for solving machine translation tasks.\n",
        "Below, we will create a Seq2Seq network that uses Transformer. The network\n",
        "consists of three parts. First part is the embedding layer. This layer converts tensor of input indices\n",
        "into corresponding tensor of input embeddings. These embedding are further augmented with positional\n",
        "encodings to provide position information of input tokens to the model. The second part is the\n",
        "actual [Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)_ model.\n",
        "Finally, the output of the Transformer model is passed through linear layer\n",
        "that gives unnormalized probabilities for each token in the target language.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "During training, we need a subsequent word mask that will prevent the model from looking into\n",
        "the future words when making predictions. We will also need masks to hide\n",
        "source and target padding tokens. Below, let's define a function that will take care of both.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now define the parameters of our model and instantiate the same. Below, we also\n",
        "define our loss function which is the cross-entropy loss and the optimizer used for training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collation\n",
        "\n",
        "As seen in the ``Data Sourcing and Processing`` section, our data iterator yields a pair of raw strings.\n",
        "We need to convert these string pairs into the batched tensors that can be processed by our ``Seq2Seq`` network\n",
        "defined previously. Below we define our collate function that converts a batch of raw strings into batch tensors that\n",
        "can be fed directly into our model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define training and evaluation loop that will be called for each\n",
        "epoch.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(train_dataloader))\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(val_dataloader))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have all the ingredients to train our model. Let's do it!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pili/anaconda3/envs/scia/lib/python3.11/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/home/pili/anaconda3/envs/scia/lib/python3.11/site-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train loss: 5.343, Val loss: 4.109, Epoch time = 39.404s\n",
            "Epoch: 2, Train loss: 3.762, Val loss: 3.312, Epoch time = 37.973s\n",
            "Epoch: 3, Train loss: 3.161, Val loss: 2.891, Epoch time = 38.304s\n",
            "Epoch: 4, Train loss: 2.768, Val loss: 2.635, Epoch time = 37.722s\n",
            "Epoch: 5, Train loss: 2.480, Val loss: 2.450, Epoch time = 37.362s\n",
            "Epoch: 6, Train loss: 2.249, Val loss: 2.313, Epoch time = 38.036s\n",
            "Epoch: 7, Train loss: 2.056, Val loss: 2.196, Epoch time = 38.372s\n",
            "Epoch: 8, Train loss: 1.893, Val loss: 2.106, Epoch time = 39.752s\n",
            "Epoch: 9, Train loss: 1.757, Val loss: 2.044, Epoch time = 38.533s\n",
            "Epoch: 10, Train loss: 1.630, Val loss: 1.998, Epoch time = 38.503s\n",
            "Epoch: 11, Train loss: 1.520, Val loss: 1.963, Epoch time = 38.733s\n",
            "Epoch: 12, Train loss: 1.418, Val loss: 1.938, Epoch time = 38.531s\n",
            "Epoch: 13, Train loss: 1.326, Val loss: 1.935, Epoch time = 38.633s\n",
            "Epoch: 14, Train loss: 1.244, Val loss: 1.934, Epoch time = 39.303s\n",
            "Epoch: 15, Train loss: 1.173, Val loss: 1.952, Epoch time = 39.166s\n",
            "Epoch: 16, Train loss: 1.106, Val loss: 1.914, Epoch time = 39.675s\n",
            "Epoch: 17, Train loss: 1.039, Val loss: 1.919, Epoch time = 38.950s\n",
            "Epoch: 18, Train loss: 0.978, Val loss: 1.932, Epoch time = 39.669s\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 18\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the model\n",
        "torch.save(transformer.state_dict(), './transformer.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load the model\n",
        "transformer.load_state_dict(torch.load('./transformer.pth', map_location=torch.device(DEVICE)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def greedy_translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " A group of people standing in front of an igloo . \n",
            " A man in a blue shirt is standing on a ladder cleaning a window . \n",
            " A young girl in a karate uniform is practicing a stick with a stick . \n"
          ]
        }
      ],
      "source": [
        "print(greedy_translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n",
        "print(greedy_translate(transformer, \"Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster .\"))\n",
        "print(greedy_translate(transformer, \"Ein junges Mädchen in einem Karateanzug bricht einen Stock mit einem Tritt .\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Theoretical questions\n",
        "\n",
        "Now that the tutorial is complete and that we have our greedy translator let's answer the following questions :\n",
        "\n",
        "### (1) In the positional encoding, why are we using a combination of sinus and cosinus?\n",
        "First of all, positional encodings are used to provide the model with information about the relative positions of tokens in the input sequence because Transformer does not have notion of word order or position. With it the Transformer model can capture the sequential relationships between tokens and leverage this information during the self-attention mechanism.\n",
        "\n",
        "But, encoding using only one type of function (sine or cosine) would result in a biased encoding pattern.\n",
        "\n",
        "By using both sine and cosine functions, the positional encoding can capture different positional patterns. Both functions vary in a wave-like manner and are shifted in phase. So these two functions together help to encode different positional relationships between tokens.\n",
        "\n",
        "The high-frequency components emanating from these functions capture small details while the low-frequency components capture larger positional relationships. So, with both functions we create a rich representation of token positions in the input sequence, enabling the transformer model to effectively use positional information during the translation process.\n",
        "\n",
        "### (2) In the Seq2SeqTransformer class, what is the parameter nhead for?\n",
        "\n",
        "The parameter nhead stands for the number of attention heads in the Transformer model.\n",
        "\n",
        "As seen in class the attention mechanism in the Transformer architecture involves computing self-attention, where we capture dependencies and relationships betwwen encoding and decoding. This operation is performed by using the model's hidden layers now called attention heads.\n",
        "\n",
        "So, the nhead parameter determines the number of attention heads used in both the encoder and decoder hidden layers of the Transformer. Each head are used to capture different types of relationships and patterns inside the model so that we can use all the information we can. Instead of only using the end sequence and have a biased attention mechanism.\n",
        "\n",
        "As nhead gets bigger it allows for more complex interactions between tokens. But, it comes at a great cost of computational efficiency. We then have to find a balance between these two concepts.\n",
        "\n",
        "### (3) In the Seq2SeqTransformer class, what is the point of the generator ?\n",
        "\n",
        "The generator is used as the final layer of the model. It is responsible for generating the output predictions based on the transformed inputs.\n",
        "\n",
        "In our case it is defined as `nn.Linear(emb_size, tgt_vocab_size)`. It is a linear layer that projects the input tensor from the embedding size to the size of the target vocabulary.\n",
        "\n",
        "When using the generator, the model obtains logits (raw scores before any activation function) for each word in the target vocabulary. These logits represent the model's predictions for the likelihood of each word being the next token in the translated sequence.\n",
        "\n",
        "It can be then passed through a softmax function to obtain a probability distribution over the target vocabulary, allowing for the selection of the most likely next word !\n",
        "\n",
        "### (4) Describe the goal of the create_mask function. Why does it handle differently the source and target masks?\n",
        "\n",
        "The goal of the `create_mask` function is to generate masks that are used in the attention mechanism of the Transformer model.\n",
        "By handling the source and target masks differently, it ensures that the attention mechanism in the Transformer model is used correctly for both the source and target sequences.\n",
        "There are 4 types of masks : \n",
        "- The source mask (`src_mask`) allows for finding dependencies between positions in the input sequence\n",
        "- The target mask (`tgt_mask`) keeps the promise that a token only attends to previous tokens in the target sequence.\n",
        "- The padding masks (`src_padding_mask` and `tgt_padding_mask`) exclude padding token (`<pad>`) from the attention mechanism to avoid unnecessary computations and maintain the integrity of the model's attention weights."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decoding functions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's make another program such as the greedy translator but using a <u>top-k sampling with temperature :</u>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def topk_decode(model, src, src_mask, max_len, start_symbol, k=2, temperature=1.0):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "\n",
        "        # topk\n",
        "        topk_prob, topk_idx = torch.topk(prob, k=k, dim=1)\n",
        "        topk_prob = topk_prob.squeeze()\n",
        "        topk_idx = topk_idx.squeeze()\n",
        "\n",
        "        # convert topk_prob to float64 to avoid overflow\n",
        "        topk_prob = topk_prob.type(torch.float64)\n",
        "\n",
        "        # temperature\n",
        "        sum_prob = torch.sum(torch.exp(topk_prob / temperature))\n",
        "        topk_prob = torch.exp(topk_prob / temperature) / sum_prob\n",
        "\n",
        "        # take one of the topk idx based on prob\n",
        "        idx_next_word = torch.multinomial(topk_prob, 1)\n",
        "\n",
        "        next_word = topk_idx[idx_next_word].item()\n",
        "\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "\n",
        "    return ys\n",
        "\n",
        "\n",
        "def topk_translate(model: torch.nn.Module, src_sentence: str, k=2, temperature=1.0):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = topk_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, k=k, temperature=temperature).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " A group of people stand in front of an igloo . \n",
            " A man in a blue shirt is standing on a ladder cleaning a window . \n",
            " A young girl in a karate uniform is practicing a stick with a stick . \n"
          ]
        }
      ],
      "source": [
        "print(topk_translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=10, temperature=0.1))\n",
        "print(topk_translate(transformer, \"Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster .\", k=10, temperature=0.1))\n",
        "print(topk_translate(transformer, \"Ein junges Mädchen in einem Karateanzug bricht einen Stock mit einem Tritt .\", k=10, temperature=0.1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And using a <u>top-p sampling with temperature :</u>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def topp_decode(model, src, src_mask, max_len, start_symbol, p=0.75, temperature=1.0):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "\n",
        "        # topp\n",
        "        sorted_prob, sorted_idx = torch.sort(prob, descending=True)\n",
        "        cumsum_prob = torch.cumsum(sorted_prob, dim=1)\n",
        "\n",
        "        # find the last index where cumsum_prob < p\n",
        "        idx = torch.where(cumsum_prob < p, cumsum_prob, torch.ones_like(cumsum_prob))\n",
        "        idx = torch.nonzero(idx == 1.0)\n",
        "\n",
        "        # if all cumsum_prob > p, then idx is empty\n",
        "        if len(idx) == 0:\n",
        "            idx = torch.tensor([[0, 0]])\n",
        "\n",
        "        # get the last index\n",
        "        idx = idx[-1, 1]\n",
        "\n",
        "        # get the top p idx\n",
        "        top_p_idx = sorted_idx[:, :idx+1]\n",
        "\n",
        "        # convert top_p_idx to float64 to avoid overflow\n",
        "        top_p_prob = sorted_prob[:, :idx+1].type(torch.float64)\n",
        "\n",
        "        top_p_idx = top_p_idx.squeeze()\n",
        "        top_p_prob = top_p_prob.squeeze()\n",
        "\n",
        "        # temperature\n",
        "        sum_prob = torch.sum(torch.exp(top_p_prob / temperature))\n",
        "        top_p_prob = torch.exp(top_p_prob / temperature) / sum_prob\n",
        "\n",
        "        # take one of the top p idx based on prob\n",
        "        idx_next_word = torch.multinomial(top_p_prob, 1)\n",
        "\n",
        "        next_word = top_p_idx[idx_next_word].item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "\n",
        "    return ys\n",
        "\n",
        "\n",
        "def topp_translate(model: torch.nn.Module, src_sentence: str, p=0.75, temperature=1.0):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = topp_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, p=p, temperature=temperature).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " A group of people stand in front an igloo . \n",
            " A man in a blue shirt is standing on a ladder cleaning a window . \n",
            " A young girl in a karate uniform is practicing a stick with a stick . \n"
          ]
        }
      ],
      "source": [
        "print(topp_translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p=0.75, temperature=0.1))\n",
        "print(topp_translate(transformer, \"Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster .\", p=0.75, temperature=0.1))\n",
        "print(topp_translate(transformer, \"Ein junges Mädchen in einem Karateanzug bricht einen Stock mit einem Tritt .\", p=0.75, temperature=0.1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ok now that we have 3 different translators let's compare them qualitatively as we play with the k, p and temperature parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "k=[2,5,10,20,50]\n",
        "p=[0.1,0.5,0.75,2]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First of all let's explain those 3 parameters :  \n",
        "- Temperature  \n",
        "    The temperature parameter affects the randomness of the generated output during sampling. It scales the logits to adjust the shape of the probability distribution. A higher temperature (>=1.0) increases randomness and encourages more exploration, resulting in diverse output. Conversely, a lower temperature (<=0.5) reduces randomness, leading to more focused and deterministic output.  \n",
        "  \n",
        "- Top-k sampling (parameter k)  \n",
        "    The parameter k determines the size of the \"top-k\" set. Instead of considering the entire vocabulary, the model only considers the k most likely tokens based on their probabilities.  \n",
        "\n",
        "- Top-p (nucleus) sampling (parameter p)  \n",
        "    This method limits the candidate tokens based on a cumulative probability threshold. The parameter p determines the threshold, where tokens with cumulative probabilities higher than p are considered. This allows for dynamic control over the number of candidate tokens and helps to maintain diversity while avoiding extremely unlikely or low-quality outputs.  \n",
        "\n",
        " \n",
        "Now, if we use the temperature equal to 0.1 we will see practically always the same result for the three methods like so : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good translation : A group of people stand in front of an igloo.\n",
            "Greedy translate :  A group of people standing in front of an igloo . \n",
            "Top-k translate :\n",
            "k= 2 :  A group of people stand in front an igloo . \n",
            "k= 5 :  A group of people stand in front an igloo . \n",
            "k= 10 :  A group of people stand in front an igloo . \n",
            "k= 20 :  A group of people standing in front of an igloo . \n",
            "k= 50 :  A group of people stand in front of an igloo . \n",
            "Top-p translate :\n",
            "p= 0.1 :  A group of people standing in front of an igloo . \n",
            "p= 0.5 :  A group of people standing in front of an igloo . \n",
            "p= 0.75 :  A group of people stand in front an igloo . \n",
            "p= 2 :  A group of people stand in front an igloo . \n"
          ]
        }
      ],
      "source": [
        "print(\"Good translation : A group of people stand in front of an igloo.\")\n",
        "print(\"Greedy translate :\", greedy_translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n",
        "print(\"Top-k translate :\")\n",
        "for i in k:\n",
        "    print(\"k=\",i,\":\",topk_translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=i, temperature=0.1))\n",
        "print(\"Top-p translate :\")\n",
        "for i in p:\n",
        "    print(\"p=\",i,\":\",topp_translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p=i, temperature=0.1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see if we have a low temperature the randomness is drastically reduced and it leaves us with deterministic outputs.  \n",
        "\n",
        "Let's try with a temperature equal to `1.0` for three different examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First translate : A group of people stand in front of an igloo.\n",
            "Greedy translate :  A group of people standing in front of an igloo . \n",
            "Top-k translate :\n",
            "k= 2 :  A group of people standing in front of an igloo \n",
            "k= 5 :  A group of people are standing in front an abandoned machines . \n",
            "k= 10 :  A group of people stand in front of an auditorium . \n",
            "k= 20 :  A group of people standing in front of an igloo \n",
            "k= 50 :  A group of people in an igloo \n",
            "Top-p translate :\n",
            "p= 0.1 :  A group of people , standing in front of an pane . \n",
            "p= 0.5 :  A group of people stand in front of an Ebay too ear . \n",
            "p= 0.75 :  A group of people stand in front an interested machines . \n",
            "p= 2 :  A group of people are standing in front of an forestry . \n"
          ]
        }
      ],
      "source": [
        "print(\"First translate : A group of people stand in front of an igloo.\")\n",
        "print(\"Greedy translate :\", greedy_translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n",
        "print(\"Top-k translate :\")\n",
        "for i in k:\n",
        "    print(\"k=\",i,\":\",topk_translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=i))\n",
        "print(\"Top-p translate :\")\n",
        "for i in p:\n",
        "    print(\"p=\",i,\":\",topp_translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p=i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Second translate : A man in a blue shirt is standing on a ladder cleaning a window.\n",
            "Greedy translate :  A man in a blue shirt is standing on a ladder cleaning a window . \n",
            "Top-k translate :\n",
            "k= 2 :  A man wearing a blue shirt stands on a ladder cleaning a window . \n",
            "k= 5 :  Man in blue shirt standing up on a ladder cleaning a window . \n",
            "k= 10 :  A man in a blue shirt stands on a ladder cleaning a window . \n",
            "k= 20 :  A man in a blue shirt is on a ladder cleaning a window . \n",
            "k= 50 :  A man in a blue shirt is standing on a ladder washing a window . \n",
            "Top-p translate :\n",
            "p= 0.1 :  A man in a blue shirt stands on a ladder cleaning a window . \n",
            "p= 0.5 :  A man in a blue shirt is standing on a ladder cleaning a window . \n",
            "p= 0.75 :  A man in a blue shirt is on a ladder cleaning a window . \n",
            "p= 2 :  Man in a blue shirt standing on a ladder cleaning a window . \n"
          ]
        }
      ],
      "source": [
        "print(\"Second translate : A man in a blue shirt is standing on a ladder cleaning a window.\")\n",
        "print(\"Greedy translate :\", greedy_translate(transformer, \"Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster .\"))\n",
        "print(\"Top-k translate :\")\n",
        "for i in k:\n",
        "    print(\"k=\",i,\":\",topk_translate(transformer, \"Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster .\", k=i))\n",
        "print(\"Top-p translate :\")\n",
        "for i in p:\n",
        "    print(\"p=\",i,\":\",topp_translate(transformer, \"Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster .\", p=i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Third translate: A young girl in a karate suit breaks a stick with a kick.\n",
            "Greedy translate :  A young girl in a karate uniform is practicing a stick with a stick . \n",
            "Top-k translate :\n",
            "k= 2 :  A young girl in a karate karate uniform uses a stick . \n",
            "k= 5 :  A young girl in a karate uniform is parasailing with a stick . \n",
            "k= 10 :  A young girl in a karate uniform is flying with a stick in a kick . \n",
            "k= 20 :  A young girl dressed in a karate karate uniform is following with a stick . \n",
            "k= 50 :  A young girl in a karate uniform is following a stick with a male kick . \n",
            "Top-p translate :\n",
            "p= 0.1 :  Young girl in a military uniform is filming a stick in a game . \n",
            "p= 0.5 :  Young girl in a karate uniform gets excited with a stick in it . \n",
            "p= 0.75 :  Young girl in a karate uniform surfs with a stick in a karate . \n",
            "p= 2 :  A young girl in a karate uniform uses a stick with a stick . \n"
          ]
        }
      ],
      "source": [
        "print(\"Third translate: A young girl in a karate suit breaks a stick with a kick.\")\n",
        "print(\"Greedy translate :\", greedy_translate(transformer, \"Ein junges Mädchen in einem Karateanzug bricht einen Stock mit einem Tritt .\"))\n",
        "print(\"Top-k translate :\")\n",
        "for i in k:\n",
        "    print(\"k=\",i,\":\",topk_translate(transformer, \"Ein junges Mädchen in einem Karateanzug bricht einen Stock mit einem Tritt .\", k=i))\n",
        "print(\"Top-p translate :\")\n",
        "for i in p:\n",
        "    print(\"p=\",i,\":\",topp_translate(transformer, \"Ein junges Mädchen in einem Karateanzug bricht einen Stock mit einem Tritt .\", p=i))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe that the temperature is letting the translation be more random.  \n",
        "As we increase the parameter k or p the action in the sentence (end of sentence) becomes different situations that has nothing to do with the real translation. It adds diversity to the translation but in terms of qualitative results the greedy translate is the best output everytime, so keeping lower parameters in the other methods seems to be preferable to have results matching the authentic translation.  \n",
        "\n",
        "As we are curious let's see what are the results for high paramters and temperature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good translation : A group of people stand in front of an igloo.\n",
            "Greedy translate :  A group of people standing in front of an igloo . \n",
            "Top-k translate :\n",
            "k= 100 :  A group of professionally people in progress stand \n",
            "Top-p translate :\n",
            "p= 5 :  Deep manipulating well outside trailer terminals a Thriller Ices bag . \n"
          ]
        }
      ],
      "source": [
        "print(\"Good translation : A group of people stand in front of an igloo.\")\n",
        "print(\"Greedy translate :\", greedy_translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n",
        "print(\"Top-k translate :\")\n",
        "print(\"k=\",100,\":\",topk_translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=100, temperature=2.0))\n",
        "print(\"Top-p translate :\")\n",
        "print(\"p=\",5,\":\",topp_translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p=5, temperature=2.0))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, the traductions became completely out of context as the randomness and diversity are extremely high."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing BLEU score of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [06:05<00:00,  2.74it/s]\n"
          ]
        }
      ],
      "source": [
        "# Use sacreBLEU implem to evaluate the model and compare the 3 methods of decoding on the test set\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "def pull_cand_ref(data_iter, model):\n",
        "    dict = {}\n",
        "    references = []\n",
        "    candidates_greedy = []\n",
        "    candidates_topk = []\n",
        "    candidates_topp = []\n",
        "\n",
        "    for (src, tgt) in tqdm(data_iter, total=1000):\n",
        "        candidates_greedy.append(greedy_translate(model, src).split())\n",
        "        candidates_topk.append(topk_translate(model, src, k=5).split())\n",
        "        candidates_topp.append(topp_translate(model, src, p=0.75).split())\n",
        "\n",
        "        references.append([tgt.split()])\n",
        "    \n",
        "    dict[\"greedy\"] = candidates_greedy\n",
        "    dict[\"topk\"] = candidates_topk\n",
        "    dict[\"topp\"] = candidates_topp\n",
        "    dict[\"refs\"] = references\n",
        "\n",
        "    return dict\n",
        "\n",
        "dict_corpus = pull_cand_ref(test_iter, transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU score on greedy decoding :\n",
            "BLEU score = 29.15%\n",
            "BLEU score on top-k decoding with k=5 and temperature=1:\n",
            "BLEU score = 24.30%\n",
            "BLEU score on top-p decoding with p=0.75 and temperature=1:\n",
            "BLEU score = 22.93%\n"
          ]
        }
      ],
      "source": [
        "# Get BLEU score on greedy decoding\n",
        "print(\"BLEU score on greedy decoding :\")\n",
        "print(f\"BLEU score = {bleu_score(dict_corpus['greedy'], dict_corpus['refs'])*100:.2f}%\")\n",
        "\n",
        "# Get BLEU score on top-k decoding\n",
        "print(\"BLEU score on top-k decoding with k=5 and temperature=1:\")\n",
        "print(f\"BLEU score = {bleu_score(dict_corpus['topk'], dict_corpus['refs'])*100:.2f}%\")\n",
        "\n",
        "# Get BLEU score on top-p decoding\n",
        "print(\"BLEU score on top-p decoding with p=0.75 and temperature=1:\")\n",
        "print(f\"BLEU score = {bleu_score(dict_corpus['topp'], dict_corpus['refs'])*100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As stated earlier the greedy decoder has a better translating efficiency than the other functions."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
